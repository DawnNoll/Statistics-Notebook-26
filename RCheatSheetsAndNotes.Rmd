---
title: "R Cheat Sheets & Notes"
output:
  html_document:
    code_folding: hide
---

## Cheat Sheets

* [R Colors](https://sape.inf.usi.ch/quick-reference/ggplot2/colour)

* [Posit/RStudio Cheat Sheets](https://rstudio.github.io/cheatsheets/html/posit-team.html)

* [R Base Commands Cheat Sheet](https://iqss.github.io/dss-workshops/R/Rintro/base-r-cheat-sheet.pdf)

* [Keyboard Shortcuts](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts)

* [Quarto Cheat Sheet](https://rstudio.github.io/cheatsheets/html/quarto.html)

* [ggplot2 Cheat Sheet](https://ctl.duke.edu/wp-content/uploads/2020/07/R_ggplot2_cheatsheet.pdf)

* [Math 221 Textbook](https://byuistats.github.io/BYUI_M221_Book/Lesson01.html)

* [Sampling Distribution Applet](https://onlinestatbook.com/stat_sim/sampling_dist/index.html)

## Test Tips

Read output carefully so you understand what it's saying! For quartemile time (qsec) a lower number is  better.<br>
Read carefully so you don't miss info in the questions.<br>
Make sure to check whether the answer is positive or negative.<br>
Make sure you put a zero before the decimal in Canvas.<br>
Make sure you hit Ctrl+enter
For SSE, etc load them into named variables, so it's easier to work with them ie, SSR/SSTO for R^2

## Modules

### 01 Linear Regression
$\beta_0, \beta_1, \sigma^2$ are for the group of dots.
The $i$'s are for the individual.
Since we can't know the errors, we estimated them with the residual.

Three main elements to the mathematical model of regression.

*  The true, unobservable line of the estimated value of the population (regression relation), provides the average Y-value-the expected value of Y:<br> 
$ \underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\ $
*  The dots (regression plus error term), $Y_i$, $\epsilon_i$ allows each individual $i$ to deviate from the line. $Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}} + \underbrace{\epsilon_i}_\text{error term} \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)$
*  The estimated line (the line we get from the sample data) 
$\hat{Y}_i$ is the estimated equation and is interpreted as the estimated average $Y$ for any $X$ $\underbrace{\hat{Y}_i}_{\substack{\text{estimated mean} \\ \text{y-value}}} = \underbrace{b_0 + b_1 X_i}_\text{estimated regression equation}$

$E\{Y\}$ true mean Y <br>
$Y_i$ the dots (add error term to above), the regression model <br>
$\hat{Y}_i = b_0 + b_1 X_i$, fitted line, lmObject$fitted.values <br>
$b_0$ Estimated intercept, 	b_0 <- mean(Y) - b_1*mean(X) <br>
$b_1$ Estimated slope, b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) <br>
$r_i$ residual-eye, distance of dot from line, lmObject's residuals <br>
$\sigma^2$ variance of $\epsilon_i$ <br>

Standard error = estimated standard deviation = the residual standard error in summary
An increase of 1,000 lbs in the weight of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles.

The interpretation of β1 is the amount of increase (or decrease) in the average y-value, denoted E{Y}, per unit change in X. It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”.

In the summary, the sigma is residual standard error and the r2 is Multiple R squared

##### Class Code lm
```{r, eval=FALSE}
# most important for week 1
mylm <- lm(Y ~ x, data=airquality)
summary(mylm)
predict(mylm, data.frame(Wind=19))

# actual class code
library(tidyverse)
View(airquality)
?airquality

# Basics
library(mosaic) #favstats
favstats(airquality$Temp)
mean(airquality$Temp)
sd(airquality$Temp)

# Histograms
hist(airquality$Temp)

ggplot(airquality, aes(x=Temp)) +
geom_histogram(binwidth=5, fill="skyblue", color="skyblue4") +
labs(title="Maximum daily...", subtitle = "May to September 1973", 
     x="Temperature in Degrees F", y="Number of Days in Temperature Range")

# Boxplot 
boxplot(Temp ~ Month, data=airquality)

ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot()

# Scatterplot
mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)
predict(mylm, data.frame(Wind=19))

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x)

favstats(Temp ~ Month, data=airquality)
ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot(fill="skyblue", color="skyblue3") + 
  labs(title="Maximum daily temperature in degrees Fahrenheit at La Guardia Airport, NY, USA",
       subtitle="May to September 1973",
       y="Temperature in degrees F", x="Month of the Year")


library(tidyverse)

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point(fill="skyblue", pch=21, color="skyblue4") + 
  geom_smooth(method="lm", se=F, formula=y~x, color="skyblue4") + 
  labs(title="Max...", subtitle="May...", 
       y="Temp...", x="Average...")

mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)






set.seed(101) #Allows us to always get the same "random" sample
#Change to a new number to get a new sample

n <- 153 #set the sample size

X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45.

beta0 <- 89 #Our choice for the y-intercept. 

beta1 <- -1.25 #Our choice for the slope. 

sigma <- 1 #Our choice for the std. deviation of the error terms.

epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model

fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data

View(fabData) 

#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.

fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

summary(fab.lm) #Summarize your model. 

plot(y ~ x, data=fabData, ylim=c(20,100)) #Plot the data.

abline(fab.lm) #Add the estimated regression line to your plot.

# Now for something you can't do in real life... but since we created the data...

abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). 

legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n") #Add a legend to your plot specifying which line is which.
```

##### Quizzes lm

```{r, eval=FALSE}
# Assessment Quiz
# 1)
#  library(car), View(Davis), ?Davis
# Reduce the data to just the data for the males. Then perform a regression with weight as the response variable and height as the explanatory variable.
# 
# Which of the following provides the estimated average weight of males that are 180 cm tall?
dav.lm <- lm(weight ~ height, data=subset(Davis, sex=="M"))
predict(dav.lm, data.frame(height=180))

# 2) 
# View(USArrests),  ?USArrests
# Perform a regression using this data that explains the average number of murder arrests in cities (per 100,000 in 1973) using the number of assault arrests (per 100,000) in a city.
#
# Select the answer that provides the correct estimate of B1 in the formula:for the USArrests regression described above.
lm(Murder ~ Assault, data = USArrests) # Choose the assault estimate

#3)
# Which of the following statements is a correct statement about the graphic shown below?
#
# Note: the "Line of Equality" shows where the line would need to be if the reported heights were equal (on average) to the actual measured heights. Dots that fall on this line show men that knew their actual height before they were officially measure
# 3 Answer (note image shows estimated line getting closer to the line of equality as heights increase)
# The average actual height gets closer to the reported height for taller men, while shorter men seem more likely to under-report their height, on average.
```
<br>

### 02 ri...MSE
Lots of notes: 425 Analysis/Residuals.qmd or RCheatSheetsbroken (next to this one)

##### Class Code ri

```{r eval=FALSE}
# most important
ri <- mylm$residuals
sse <- sum(mylm$res^2 ) # SSE is the unexplained variability
ssr <- sum( (mylm$fit - mean(data$Y))^2) # SSR is the explained variablility
ssto <- sum((data$Y - mean(data$Y))^2) # SSTO, variability from ybar
r2 <-ssr/ssto # R2 is the ration betwee explanation and total SSR/SSTO
sqrt(r2) # r is square root of r^2

# how to index dataset
cars[23, ] # row
cars.lm$residuals[23] # corresponding residual
cars$dist[23] - cars.lm$fitted.values[23] #alt to calc single residual

###
# Monday
cars2 <- cars[sample(1:50, 10), ]
View(cars2)

library(tidyverse)

carslm <- lm(dist ~ speed, data=cars2)
summary(carslm)
predict(carslm, data.frame(speed=17.5))

# add line segments
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_segment(aes(x=17.5, xend=17.5, y=45.297, yend=50))


# add squares
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_rect(aes(xmin=17.5, xmax=17.5+.65, ymin=45.297, ymax=50), alpha=0.1) + 
  geom_rect(aes(xmin=speed, xmax=speed+carslm$residuals*.15, ymin=dist, ymax=carslm$fitted.values), alpha=0.1, fill="blue")

install.packages("plotly")
library(plotly)

x = c(5, 15, 2, 29, 35, 24, 25, 39) 
sum(x)
sum( (1:6)^2 ) 
sum(x^2)
mn <- mean(x)
var(x)
dist <-(x-mn)
dist
sum(dist^2)/7
mean(cars$dist)
sum((1:3)^2)

# Wednesday
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
cars2 <- cbind(rbind(cars, 
                     cbind(speed=rep(0,50), dist=cars$dist),
                     cbind(speed=cars$speed, dist=cars.lm$fitted.values)),
               frame = rep(c(2,1,3), each=50))
plot_ly(cars2,
        x = ~speed,
        y = ~dist,
        frame = ~frame,
        type = 'scatter',
        mode = 'markers',
        showlegend = F,
        marker=list(color="firebrick")) %>%
  layout(title="Stopping Distance of 1920's Vehicles\n (cars data set)",
         xaxis=list(title="Vehicles Speed in mph (speed)"),
         yaxis=list(title="Stopping Distance in Feet (dist)")) %>%
  add_segments(x = 0, xend = 25, y = mean(cars$dist), yend = mean(cars$dist), line=list(color="gray", dash="dash", width=1), inherit=FALSE, name=TeX("\\bar{Y}"), showlegend=T) %>%
  add_segments(x = 0, xend = 25, y = sum(coef(cars.lm)*c(1,0)), yend = sum(coef(cars.lm)*c(1,25)), line=list(color="darkgray", width=1), inherit=FALSE, name=TeX("\\hat{Y}"), showlegend=T) %>%
  config(mathjax = 'cdn') %>%
  animation_opts(frame=2000, transition=1000, redraw=T)

# Friday
library(tidyverse)
View(diamonds)
plot(price ~ carat, data=diamonds)

diamonds.lm <- lm(price ~ carat, data=diamonds)
summary(diamonds.lm)

abline(diamonds.lm, col="green", lwd=2)

plot(dist ~ speed, data=cars)
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
abline(cars.lm)
points(dist ~ speed, data=cars[23,], col="orange", pch=16)
```

##### Quizzes ri

```{r eval=FALSE}
# Mobius
# wt
mtwt.lm <- lm(mpg~wt, mtcars)
summary(mtwt.lm)

ggplot(mtcars, aes(y=mpg, x=wt)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtwt.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtwt.ssr <- sum( (mtwt.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtwt.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(wtr2 <-mtwt.ssr/mtwt.ssto)


# r is square root of r^2
sqrt(wtr2)

predict(mtwt.lm, data.frame(wt=3*365))

par(mfrow=c(1,3))
plot(mtwt.lm, which = 1:2)
plot(mtwt.lm$residuals)


# cyl
View(mtcars)
mtcyl.lm <- lm(mpg~cyl, mtcars)
summary(mtcyl.lm)
mean(mtcars$mpg)

ggplot(mtcars, aes(y=mpg, x=cyl)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtcyl.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtcyl.ssr <- sum( (mtcyl.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtcyl.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(cylr2 <-mtcyl.ssr/mtcyl.ssto)


# r is square root of r^2
sqrt(cylr2)

predict(mtcyl.lm, data.frame(cyl=3*365))

par(mfrow=c(1,3))
plot(mtcyl.lm, which = 1:2)
plot(mtcyl.lm$residuals)


# Assessment: Residuals, Sums of Squares, and R squared
# 1) A regression was performed for a sample of n = 5 data points.
# The y-values of the regression are: 3.78, 6.08, 6.65, 9.25, and 9.92.
# The residuals from the regression are: -0.266, 0.489, -0.486, 0.569, and -0.306.
# What is the R-squared value for this regression?
y <- c(3.78, 6.08, 6.65, 9.25, 9.92)

SSTO <- sum( (y - mean(y))^2 )
#SSTO = 24.83372

res <- c(-0.266, 0.489, -0.486, 0.569, -0.306)
SSE <- sum(res^2)
#SSE = 0.96347

R2 = 1 - SSE/SSTO
#R2 = 0.9612032

# 2) This data can be used to show that the displacement of the engine (disp)
# is positively correlated with the weight of the vehicle (wt).
# Apparently, this means disp is the Y and wt is the x
# Don't use mylm unless you've cleared everything or use something descriptive like clm for cars mylm
mt.lm <- lm(disp ~ wt, data=mtcars)
summary(mt.lm)
# Multiple R-squared: 0.7885, Adjusted R-squared: 0.7815  <- (multiple R-squared is R^2)

# 3)
# A certain statistics teacher at BYU-Idaho drives a 2001 Nissan Sentra that weighs
# approximately 2,700 lbs and currently gets only 21 mpg.
# Based on the regression you performed, how many mpg above or below average is this vehicle?
#   Perform a regression of mpg means mpg is the Y and wt is the x
mt.lm <- lm(mpg ~ wt, data=mtcars) #perform the regression
plot(mpg ~ wt, data=mtcars) #draw the regression (not needed, but nice)
abline(mt.lm) #add the regression line (not needed, but nice)
points(2.7, 21, pch=16, col="skyblue") #add the Nissan Sentra value (not needed, but nice)
predict(mt.lm, data.frame(wt=2.7)) #get predicted value for Nissan Sentra
1
22.85505
lines(c(2.7, 2.7), c(21, 22.85505), col="skyblue") #add residual line (not needed, but nice)
myresidual <- 21 - 22.85505 #calculate difference between Y and Y-hat

> myresidual
[1] -1.85505
```
<br>



### 03 Diagnostics & Transformations

##### Residual Plots & Regression Assumptions & Transformations

*  residuals vs. fitted-values (which=1), most important (for assump 1 straight line,3 no pattern)
*  Q-Q Plot of Residuals (which=2), least important (for assump 2, close to line) or library(car), qqPlot(lm$residuals) has guides
*  Residuals vs Order (lm$residals), second most important (for assump 5, no pattern)<br>


Five Assumptions:

1)  The regression relation between Y and X is linear, plot(mylm, which=1)<br>
*res v fit (red line straight) If bent results are meaningless--it affects everything.*<br>
Nothing can be trusted if the linearity assumption is violated.
2)  The error terms are normally distributed with E{ϵi}=0. QQ checks normality<br>
plot(mylm, which=2)<br>
*dots should be close to on the line, watch tails*<br>
If the error terms are not normal, the slope and intercept are often still meaningful (when the independence assumption is violated), but the residual standard error could be unnecessarily large in this case. 
3)  The variance of the error terms is constant over all X values, plot(mylm, which=1)<br>
*no pattern in plot?*<br>
Distance should be constant on res v fit. When variance of the error term changes across the regression, the regression approximates the “average variance” of the errors because the regression is still assuming the variance is constant across the regression. The estimates of the slope and intercept are still typically quite good, and can be used for interpretation. The residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end.<br> 
4)  The X values can be considered fixed and measured without error.
5)  The error terms are independent. plot(mylm$residuals, ylab="Residuals")<br>
*res v order should have no pattern*<br>
While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error is unnecessarily large in this case.Can be checked when the data has natural ordering to it eg., time series often break it.<br>

Also--WATCH FOR OUTLIERS, While outliers do not violate any of the regression assumptions, they do pose substantial difficulties for the least squares regression estimates of the slope and intercept.

$$Y_i = \underbrace{\beta_0 + \beta_1 \overbrace{X_i}^\text{#4}}_{\text{#1}} + \epsilon_i \quad \text{where} \ \overbrace{\epsilon_i \sim}^\text{#5} \overbrace{N(0}^\text{#2}, \overbrace{\sigma^2}^\text{#3})$$


How does plot change when we look at residuals vs fitted-values Which is the y
How check assumption 4 Does it make sense th.n having x fixed is workes
Explain the difficulties that arise when there is an outlier present. (how do outliers skew) biases the slope

##### Transformations
For week 03 assign, tell what BoxCox says, and whether you used it.
Show math, my y` is this, my belief on the prime space is this, resulting equation back in original units is this.

the log changes the way I see the data, changes the lens, changes my perspective. 
Viewing a histogram of the islands data and log islands data--you can see more detail with the log view

Y transformations can be found using boxCox
boxCox(lm)     # from library(car), log-likelihood
boxCox is a suggestion not a rule or law
If no y transform needed, bCox will be around 1

X transformations are more difficult to find because boxCox doesn't work. If linearity is violated, but variance across residuals is constant (both are on rvf which=1)

When have a choice choose log transform because it's the only one that's interpretable. Also choose wisely, for orange trees y^2 trees have no circumference until day 200, 1/y don't really start growing for years and then take over:) For youngOrange, even though log was best for current data it may make less sense for a real set of random trees, y might be the best of these options. Make sure it's logical.

He used the ` prime mark for optimus prime ie. it's been transformed. So Y`i = b0+b1 from other space
Run a lm of transform Y, then summarize : summary(lm.t)
save coef

Interesting note from gpt
When you back transform, this is actually estimating the median of the expression, not the mean, unless you apply a bias correction (interesting technicality)
Mean Squared Error
$$\begin{equation}
  s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\widehat{Y}_i)^2}{n-2} = \frac{\sum r_i^2}{n-2}
\end{equation}$$

##### Class Code Diag

```{r, eval=FALSE}
# most important code
par(mfrow=c(1,3))
plot(mylm, which = 1:2) #1,3 & 2
plot(mylm$residuals) #5
boxCox(lm) # lambda for Y

###
# Beg week- Diagnosing Models
view(mtcars)
?mtcars
lm.mt <- lm(mpg ~ qsec, data=mtcars)
plot(mpg ~ qsec, data=mtcars)
abline(lm.mt)
plot(lm.mt, which=1)

plot(drat ~ wt, data=mtcars)
lm2 <- lm(drat ~ wt, data=mtcars)
abline(lm2)
plot(lm2, which=1)

?Loblolly
view(Loblolly)
plot(height ~ age, data=Loblolly)
lmlob <- lm(height ~ age, data=Loblolly)
abline(lmlob)
plot(lmlob, which=1)
par(mfrow=c(1,3))
plot(lmlob, which = 1:2)
plot(lmlob$residuals)

summary(lmlob)

?Orange
View(Orange)
plot(circumference ~ age, data=Orange)

#Midweek- Intro to Transformations (log)
#remember log default is natural log, so log(5000), is e^?=5000
#exp(number), is e^number=what
log(5000)
exp(8.517193)
exp(1) #2.718282

hist(islands, col="forestgreen")
hist(log(islands), col="forestgreen")
exp(4)
log(10000)

# End of week
# Graphing Transformations
library(mosaicData)
library(car)
View(Utilities)
?Utilities
# data with lm
gas.lm <- lm(gasbill ~ temp, data=Utilities)
plot(gasbill ~ temp, data=Utilities)
abline(gas.lm)
# res v fitted
plot(gas.lm, which=1) # bend in rvf, he says the outliers have notes, could be removed as bad data
#NEW
boxCox(gas.lm) # to find lambda for Y
lambda <- -2
plot(gasbill^lambda ~ temp, data=Utilities)

# test multiple lambda's to see which is best
for (lambda in c(-2, -1, -.5, -.4, -.3, -.2, -.1, .1, .25, .5, 1, 2)) {
  plot(gasbill^lambda ~ temp, data=Utilities, main = paste("lambda = ", lambda))
}


for (lambda in seq(0.5, 0.005, length.out=10)) {
  plot(gasbill^lambda ~ temp, data=Utilities, main = paste("lambda = ", lambda))
}

plot(log(gasbill) ~ temp, data=Utilities)
u.lm.t <- lm(log(gasbill) ~ temp, data=Utilities)
abline(u.lm.t)

summary(u.lm)$coef
summary(u.lm.t)$coef

exp(predict(u.lm.t, data.frame(temp=90)))
predict(u.lm, data.frame(temp=90))


plot(gasbill ~ temp, data=Utilities)
curve(exp(6 - 0.04*x), add=TRUE, col="skyblue")


ggplot(Utilities, aes(x=temp, y=gasbill)) + 
  geom_point() + 
  stat_function(fun=function(x) exp(6 - 0.04*x))

# OLDdata with log transformation (sqrt, sqrt had best lambda)
gas.lm.log <- lm(log(gasbill) ~ temp, data=Utilities)
b <- coef(gas.lm.log) # grab line
# untransform add line
plot(log(gasbill) ~ temp, data=Utilities)
abline(gas.lm.log)

plot(gas.lm.log, which=1)
summary(gas.lm.log)

#bring back the curved line
plot(gasbill ~ temp, data=Utilities) # base plot, to log
curve(exp(6.031885 - 0.041435*x), add=TRUE) # curve from log, but exp

plot(gasbill ~ temp, data=Utilities)
abline(gas.lm)
curve(exp(b[1] + b[2]*x), add=TRUE, col="skyblue") #curve() requires lower-case x


ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( )

  stat_function(fun=function(x) ())# not sure what this was for

# Bro Saunders code
lm.log <- lm(log(circumference) ~ age, data=Orange)
b.log <- coef(lm.log)
b.log[1]
b.log[2]

lm.sqrt <- lm(sqrt(circumference) ~ age, data=Orange)
b.sqrt <- coef(lm.sqrt)

lm.1oY <- lm(1/circumference ~ age, data=Orange)
b.1oY <- coef(lm.1oY)

lm.y <- lm(circumference ~ age, data=Orange)
b.y <- coef(lm.y)

lm.y2 <- lm(circumference^2 ~ age, data=Orange)
b.y2 <- coef(lm.y2)

lm.1oY2 <- lm(1/circumference^2 ~ age, data=Orange)
b.1oY2 <- coef(lm.1oY2)


# Base Graphic


plot(circumference ~ age, data=Orange, pch=16, col="orangered", main="Growth of Orange Trees", xlab="Age of Tree in Days", ylab="Circumference of Tree (mm)")

curve( exp(b.log[1] + b.log[2]*x), add=TRUE, col="red")
curve( (b.sqrt[1] + b.sqrt[2]*x)^2, add=TRUE, col="blue")
curve( 1/(b.1oY[1] + b.1oY[2]*x), add=TRUE, col="green")
curve( b.y[1] + b.y[2]*x, add=TRUE, col="gray")
curve( sqrt(b.y2[1] + b.y2[2]*x), add=TRUE, col="orange")
curve( 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), add=TRUE, col="forestgreen")

# ggplot Graphic

library(tidyverse)

ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  stat_function(fun=function(x) exp(b.log[1] + b.log[2]*x), aes(color="log(Y)")) + 
  stat_function(fun=function(x) (b.sqrt[1] + b.sqrt[2]*x)^2, aes(color="sqrt(Y)")) + 
  stat_function(fun=function(x) 1/(b.1oY[1] + b.1oY[2]*x), aes(color="1/Y")) + 
  stat_function(fun=function(x) b.y[1] + b.y[2]*x, aes(color="Y")) + 
  stat_function(fun=function(x) sqrt(b.y2[1] + b.y2[2]*x), aes(color="Y^2")) + 
  stat_function(fun=function(x) 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), aes(color="1/Y^2")) + 
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( ) 

library(car)
boxCox(lm.y)


YoungOrange <- filter(Orange, age < 1200)


lm.log <- lm(log(circumference) ~ age, data=YoungOrange)
b.log <- coef(lm.log)

lm.sqrt <- lm(sqrt(circumference) ~ age, data=YoungOrange)
b.sqrt <- coef(lm.sqrt)

lm.1oY <- lm(1/circumference ~ age, data=YoungOrange)
b.1oY <- coef(lm.1oY)

lm.y <- lm(circumference ~ age, data=YoungOrange)
b.y <- coef(lm.y)

lm.y2 <- lm(circumference^2 ~ age, data=YoungOrange)
b.y2 <- coef(lm.y2)

lm.1oY2 <- lm(1/circumference^2 ~ age, data=YoungOrange)
b.1oY2 <- coef(lm.1oY2)



ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  geom_vline(aes(xintercept=1200)) + 
  stat_function(fun=function(x) exp(b.log[1] + b.log[2]*x), aes(color="log(Y)")) + 
  stat_function(fun=function(x) (b.sqrt[1] + b.sqrt[2]*x)^2, aes(color="sqrt(Y)")) + 
#  stat_function(fun=function(x) 1/(b.1oY[1] + b.1oY[2]*x), aes(color="1/Y")) + 
  stat_function(fun=function(x) b.y[1] + b.y[2]*x, aes(color="Y")) + 
  stat_function(fun=function(x) sqrt(b.y2[1] + b.y2[2]*x), aes(color="Y^2")) + 
#  stat_function(fun=function(x) 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), aes(color="1/Y^2")) + 
  ylim(c(0,300)) + 
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( ) 
```


##### Quizzes Diag
``` {r, eval=FALSE}
View(Orange)
o.lm <- lm(circumference ~ age, data=Orange)
summary(o.lm)


ggplot(data=Orange, mapping=aes(y=circumference, x=age)) + geom_point(color = "orange", pch=16) +
geom_smooth(method="lm", formula=y~x, se=F) +
stat_function(fun=function(x) (b[1] + b.[2]*x)^2, aes(color="sqrt(Y)")) + 
  theme_minimal()

# sqrt(MSE)
sqrt(sum(o.lm$residuals^2)/(35-2))

plot(o.lm, which=1:2)
plot(o.lm$residuals)
# the variance is not constant, minor issues with linearity, minor issues with normality

library(car)
boxCox(o.lm)
o.sqrt.lm <- lm(sqrt(circumference) ~ age, data=Orange)
summary(o.sqrt.lm)
b <- coef(o.sqrt.lm)
t.lm <- lm((b[0] + b[1]*age)^2 ~ age, data=Orange)

ggplot(data=Orange, mapping=aes(y=sqrt(circumference), x=age)) + geom_point(color = "orange", pch=16) +
geom_smooth(method="lm", formula=y~x, se=F) +
  theme_minimal()

plot(o.sqrt.lm, which=1:2)
plot(o.sqrt.lm$residuals)

plot(circumference ~ age, data=Orange)
abline(o.lm)
curve((b[1] + b[2]*x)^2, add=TRUE)

ggplot(data=Orange, mapping=aes(y=circumference, x=age)) + geom_point(color = "orange", pch=16) +
geom_smooth(method="lm", formula=y~x, se=F) +
  
  theme_minimal()

o.pred <- predict(o.sqrt.lm, data.frame(age=500))
o.pred^2  # use predict on the transformed model and then reverse the answer
```

<br>

### 04 Hypothesis Test for Model Parameters
Sampling Distributions of Model Parameters
Important to understand: ALL STATISTICS ARE WRONG! (because we have a sample)

2 biggest problems in statistics:
1) Every sample is wrong (but they are close)
2) No 2 samples agree  
When you take a sample from the population the line varies from the truth.
The regression lines from samples are closer in the middle and further apart on the ends.

Residual Standard Error is the average spread of the dots from the line (sigma)
Multiple R-squared is the proportion of variability in Y that's explained by the regression
The Estimated Intercept, x, RSE, and R^2 tells us about our data.e The Std. Error, tvalue, and Pr (p), tell us about all possible samples around the truth. "One sample has enough DNA to recreate the whole dinosaur."

repeating (rep) a sequence (seq) is the best way to collect data for regression (create samples), you get two replicates for each place--gives the most insight for the least data possible

It can be mathematically proven the the expected value of b1 is beta1

Only 3 things affect the final variability of the slope and the intercept, sigma (not in our control), sample size, and the spread of x

MSE is the avg standard error and also a measure of the vertical variability of the dots around the line? y?

${\sigma_{b_1}}^2 = \frac{\sigma^2}{sum(X_i-\bar{X})^2}$ The variability of the slope is the variability of the y's over the variability of the x's
MSE is an approximation of sigma squared

##### Class Code Samp Dist

```{r, eval=FALSE}
### Week Start
# inline r chunk `r beta_0` can use it to populate the equation
# Sampling Distributions of Model Parameters
#for loop
N <- 512 
storage <- rep(NA, N)
storage

for (i in 1:N){
  storage[i] <- 2*i
  cat("i =", i, " and 2*i =", 2*i, " was saved in storage[", i, "]\n")
}

storage

# abline stands for add b-slope line
# geom_smooth, se=F, takes the band away, this week remove, so we can see the band the band suggests where the true line will be


library(tidyverse)

set.seed(121)


N <- 50000
beta0 <- 3
beta1 <- 2.5

sigma <- 1.2

X <- rnorm(N, 30, 5)
Y <- beta0 + beta1*X + rnorm(N, 0, sigma)

plot(Y ~ X, col=rgb(.1,.1,.1,.01), pch=16)

n <- 5
mysample1 <- sample(N, n)

points(Y[mysample1] ~ X[mysample1], col="hotpink", pch=16)
mylm1 <- lm(Y[mysample1] ~ X[mysample1])
summary(mylm1)
abline(mylm1, col="hotpink")
abline(beta0, beta1, col="green1")


n <- 5
mysample2 <- sample(N, n)

points(Y[mysample2] ~ X[mysample2], col="lemonchiffon", pch=16)
mylm2 <- lm(Y[mysample2] ~ X[mysample2])
summary(mylm2)
abline(mylm2, col="lemonchiffon")

mydata <- data.frame(Y=Y[mysample2],X=X[mysample2])

ggplot(mydata, aes(x=X, y=Y)) + 
  geom_point() + 
  geom_smooth(method="lm", formula=y~x) + 
  geom_abline(aes(intercept=beta0, slope=beta1), color="green1")


N <- 512  
storage <- rep(NA, N)
storage

for (i in 1:N){
  storage[i] <- 2*i
}

storage


N <- 5000
storage_int <- storage_slope <- rep(NA, N)

for (i in 1:N){

  #Hint 1

  n <- 40
  Xi <- rep(seq(30, 100, length.out=n/2), each=2) #n must be even.
  Yi <- 2.5 + 3*Xi + rnorm(n, 0, 1.2)

  #Hint 2

  mylm <- lm(Yi ~ Xi)
  #coef(mylm)
  storage_int[i] <- coef(mylm)[1] #intercept only
  storage_slope[i] <- coef(mylm)[2] #slope only

}

#Hint 3
hist(storage_int)
hist(storage_slope)

mean(storage_int)
mean(storage_slope)
sd(storage_int)
sd(storage_slope)

# If I increase the spread of x in the regression model, the sd decreases. 
# If I decrease the spread of x in the regression model, the sd increases.
# If I increase the sigma, the sd increases.
# If I decrease the sigma, the sd decreases.
# reducing samle size increases variability
```

##### Class Code Hyp Test
```{r, eval=FALSE, fig.height=8, fig.width=8}

```{r}
## Simulation to Show relationship between Standard Errors

##-----------------------------------------------
## Edit anything in this area... 

n <- 100 #sample size (choose a decent size sample)
# spread of x (choose a decent spread)
Xstart <- 30 #lower-bound for x-axis 
Xstop <- 100 #upper-bound for x-axis

beta_0 <- 2 #choice of true y-intercept
beta_1 <- 3.5 #choice of true slope
sigma <- 13.8 #choice of st. deviation of error terms (choose a relatively small sigma)

## End of Editable area.

X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) #Create X - this way most insight for least data 
N <- 5000 #number of times to pull a random sample (for the assignment it's 3)
# notice using b0, b1 because it's not the law, not beta
storage_b0 <- storage_b1 <- rep(NA, N)
for (i in 1:N){
  Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
  mylm <- lm(Y ~ X)
  storage_b0[i] <- coef(mylm)[1]
  storage_b1[i] <- coef(mylm)[2]
}

# only works if you're using base r
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart <- 0 #min(0,min(Y)) 
Ystop <- 500 #max(max(Y), 0)
Yrange <- Ystop - Ystart
# need plot and histograms, the rest is extra
plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col="gray",
     main="Regression Lines from many Samples\n Plus Residual Standard Deviation Lines")
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)
text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1)


for (i in 1:N){
  abline(storage_b0[i], storage_b1[i], col="darkgray")  
}
abline(beta_0, beta_1, col="green", lwd=3)
abline(beta_0+sigma, beta_1, col="green", lwd=2)
abline(beta_0-sigma, beta_1, col="green", lwd=2)
abline(beta_0+2*sigma, beta_1, col="green", lwd=1)
abline(beta_0-2*sigma, beta_1, col="green", lwd=1)
abline(beta_0+3*sigma, beta_1, col="green", lwd=.5)
abline(beta_0-3*sigma, beta_1, col="green", lwd=.5)

par(mai=c(1,.6,.5,.01))

  addnorm <- function(m,s, col="firebrick"){
    curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
    lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
    lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
    lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
    lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
    lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
    lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
    lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
    legend("topleft", legend=paste("Std. Error = ", round(s,3)), cex=0.7, bty="n")
  }
# All you really need is a histogram in storage
  h0 <- hist(storage_b0, 
             col="skyblue3", 
             main="Sampling Distribution\n Y-intercept",
             xlab=expression(paste("Estimates of ", beta[0], " from each Sample")),
             freq=FALSE, yaxt='n', ylab="")
  m0 <- mean(storage_b0)
  s0 <- sd(storage_b0)
  addnorm(m0,s0, col="green")
  
  h1 <- hist(storage_b1, 
             col="skyblue3", 
             main="Sampling Distribution\n Slope",
             xlab=expression(paste("Estimates of ", beta[1], " from each Sample")),
             freq=FALSE, yaxt='n', ylab="")
  m1 <- mean(storage_b1)
  s1 <- sd(storage_b1)
  addnorm(m1,s1, col="green")

# y-intercept distribution tells us the highest and lowest and most imp where we usually land on yaxis

cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
confint(mylm)
```
```


##### Quizzes Hyp Param

```{r, eval=FALSE}

```


<br>

## Code notes

##### Generic
Ctrl + L to clear console
Getwd()<br>
Glimpse() to see the data and it's types (good for when ? Doesn't work)<br>
In rmd ctr shft c to comment/uncomment<br>
Echo = false means the code won't show just output<br>
Double question marks tells you where<br>
Don't put install in the script cause it'll do it every time<br>
Say no to saving workspace instead save a script with everything.<br>
Shortcut for pipe is ctrl + shift + m<br>

##### Graphics
Notebook add links to rmd files (can also put line number)<br>
P + theme(text=element_text(size24))

##### Problems
Mismatched directories can sometimes cause you problems when you knit.<br> 
Session>Set Working Directory to Source file location<br>
NAs are contagious they will permeate your data, pass na.rm true to remove the nas<br>
Session > restart R because he overwrote his data<br>
Clicking on broom cleans out all the data<br>

##### Reminders
Don't do anything to the original data file make sure to assign it to a new name!!!<br>
Filter rows/select columns<br>
Colnames() to see the column names<br>
Tibble() see code for names and first few rows<br>
Easier to do test questions in base R<br>
Filter into a new data set before summarizing<br>

<a id=" Code Notes"></a>



