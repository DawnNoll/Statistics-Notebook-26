---
title: "R Cheat Sheets & Notes"
output:
  html_document:
    code_folding: hide
---

## Cheat Sheets

* [R Colors](https://sape.inf.usi.ch/quick-reference/ggplot2/colour)

* [Posit/RStudio Cheat Sheets](https://rstudio.github.io/cheatsheets/html/posit-team.html)

* [R Base Commands Cheat Sheet](https://iqss.github.io/dss-workshops/R/Rintro/base-r-cheat-sheet.pdf)

* [Keyboard Shortcuts](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts)

* [Quarto Cheat Sheet](https://rstudio.github.io/cheatsheets/html/quarto.html)

* [ggplot2 Cheat Sheet](https://ctl.duke.edu/wp-content/uploads/2020/07/R_ggplot2_cheatsheet.pdf)

## Test Tips

Read output carefully so you understand what it's saying! For quartemile time (qsec) a lower number is  better.<br>
Read carefully so you don't miss info in the questions.<br>
Make sure to check whether the answer is positive or negative.<br>
Make sure you put a zero before the decimal in Canvas.<br>
Make sure you hit Ctrl+enter
For SSE, etc load them into named variables, so it's easier to work with them ie, SSR/SSTO for R^2

## Modules

### 01 Linear Regression
$\beta_0, \beta_1, \sigma^2$ are for the group of dots.
The $i$'s are for the individual.
Since we can't know the errors, we estimated them with the residual.

Three main elements to the mathematical model of regression.

*  The true, unobservable line of the estimated value of the population (regression relation), provides the average Y-value-the expected value of Y:<br> 
$ \underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\ $
*  The dots (regression plus error term), $Y_i$, $\epsilon_i$ allows each individual $i$ to deviate from the line. $Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}} + \underbrace{\epsilon_i}_\text{error term} \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)$
*  The estimated line (the line we get from the sample data) 
$\hat{Y}_i$ is the estimated equation and is interpreted as the estimated average $Y$ for any $X$ $\underbrace{\hat{Y}_i}_{\substack{\text{estimated mean} \\ \text{y-value}}} = \underbrace{b_0 + b_1 X_i}_\text{estimated regression equation}$

$E\{Y\}$ true mean Y <br>
$Y_i$ the dots (add error term to above), the regression model <br>
$\hat{Y}_i = b_0 + b_1 X_i$, fitted line, lmObject$fitted.values <br>
$b_0$ Estimated intercept, 	b_0 <- mean(Y) - b_1*mean(X) <br>
$b_1$ Estimated slope, b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 ) <br>
$r_i$ residual-eye, distance of dot from line, lmObject's residuals <br>
$\sigma^2$ variance of $\epsilon_i$ <br>

Standard error = estimated standard deviation = the residual standard error in summary
An increase of 1,000 lbs in the weight of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles.

The interpretation of β1 is the amount of increase (or decrease) in the average y-value, denoted E{Y}, per unit change in X. It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”.

##### Class Code lm
```{r, eval=FALSE}
# most important for week 1
mylm <- lm(Y ~ x, data=airquality)
summary(mylm)
predict(mylm, data.frame(Wind=19))

# actual class code
library(tidyverse)
View(airquality)
?airquality

# Basics
library(mosaic) #favstats
favstats(airquality$Temp)
mean(airquality$Temp)
sd(airquality$Temp)

# Histograms
hist(airquality$Temp)

ggplot(airquality, aes(x=Temp)) +
geom_histogram(binwidth=5, fill="skyblue", color="skyblue4") +
labs(title="Maximum daily...", subtitle = "May to September 1973", 
     x="Temperature in Degrees F", y="Number of Days in Temperature Range")

# Boxplot 
boxplot(Temp ~ Month, data=airquality)

ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot()

# Scatterplot
mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)
predict(mylm, data.frame(Wind=19))

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x)

favstats(Temp ~ Month, data=airquality)
ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot(fill="skyblue", color="skyblue3") + 
  labs(title="Maximum daily temperature in degrees Fahrenheit at La Guardia Airport, NY, USA",
       subtitle="May to September 1973",
       y="Temperature in degrees F", x="Month of the Year")


library(tidyverse)

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point(fill="skyblue", pch=21, color="skyblue4") + 
  geom_smooth(method="lm", se=F, formula=y~x, color="skyblue4") + 
  labs(title="Max...", subtitle="May...", 
       y="Temp...", x="Average...")

mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)






set.seed(101) #Allows us to always get the same "random" sample
#Change to a new number to get a new sample

n <- 153 #set the sample size

X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45.

beta0 <- 89 #Our choice for the y-intercept. 

beta1 <- -1.25 #Our choice for the slope. 

sigma <- 1 #Our choice for the std. deviation of the error terms.

epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model

fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data

View(fabData) 

#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.

fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

summary(fab.lm) #Summarize your model. 

plot(y ~ x, data=fabData, ylim=c(20,100)) #Plot the data.

abline(fab.lm) #Add the estimated regression line to your plot.

# Now for something you can't do in real life... but since we created the data...

abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). 

legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n") #Add a legend to your plot specifying which line is which.
```

##### Quizzes lm

```{r, eval=FALSE}
# Assessment Quiz
# 1)
#  library(car), View(Davis), ?Davis
# Reduce the data to just the data for the males. Then perform a regression with weight as the response variable and height as the explanatory variable.
# 
# Which of the following provides the estimated average weight of males that are 180 cm tall?
dav.lm <- lm(weight ~ height, data=subset(Davis, sex=="M"))
predict(dav.lm, data.frame(height=180))

# 2) 
# View(USArrests),  ?USArrests
# Perform a regression using this data that explains the average number of murder arrests in cities (per 100,000 in 1973) using the number of assault arrests (per 100,000) in a city.
#
# Select the answer that provides the correct estimate of B1 in the formula:for the USArrests regression described above.
lm(Murder ~ Assault, data = USArrests) # Choose the assault estimate

#3)
# Which of the following statements is a correct statement about the graphic shown below?
#
# Note: the "Line of Equality" shows where the line would need to be if the reported heights were equal (on average) to the actual measured heights. Dots that fall on this line show men that knew their actual height before they were officially measure
# 3 Answer (note image shows estimated line getting closer to the line of equality as heights increase)
# The average actual height gets closer to the reported height for taller men, while shorter men seem more likely to under-report their height, on average.
```
<br>

### 02 ri...MSE
Lots of notes: 425 Analysis/Residuals.qmd or RCheatSheetsbroken (next to this one)

##### Class Code ri

```{r eval=FALSE}
# most important
ri <- mylm$residuals
sse <- sum(mylm$res^2 ) # SSE is the unexplained variability
ssr <- sum( (mylm$fit - mean(data$Y))^2) # SSR is the explained variablility
ssto <- sum((data$Y - mean(data$Y))^2) # SSTO, variability from ybar
r2 <-ssr/ssto # R2 is the ration betwee explanation and total SSR/SSTO
sqrt(r2) # r is square root of r^2

# how to index dataset
cars[23, ] # row
cars.lm$residuals[23] # corresponding residual
cars$dist[23] - cars.lm$fitted.values[23] #alt to calc single residual

###
# Monday
cars2 <- cars[sample(1:50, 10), ]
View(cars2)

library(tidyverse)

carslm <- lm(dist ~ speed, data=cars2)
summary(carslm)
predict(carslm, data.frame(speed=17.5))

# add line segments
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_segment(aes(x=17.5, xend=17.5, y=45.297, yend=50))


# add squares
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_rect(aes(xmin=17.5, xmax=17.5+.65, ymin=45.297, ymax=50), alpha=0.1) + 
  geom_rect(aes(xmin=speed, xmax=speed+carslm$residuals*.15, ymin=dist, ymax=carslm$fitted.values), alpha=0.1, fill="blue")

install.packages("plotly")
library(plotly)

x = c(5, 15, 2, 29, 35, 24, 25, 39) 
sum(x)
sum( (1:6)^2 ) 
sum(x^2)
mn <- mean(x)
var(x)
dist <-(x-mn)
dist
sum(dist^2)/7
mean(cars$dist)
sum((1:3)^2)

# Wednesday
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
cars2 <- cbind(rbind(cars, 
                     cbind(speed=rep(0,50), dist=cars$dist),
                     cbind(speed=cars$speed, dist=cars.lm$fitted.values)),
               frame = rep(c(2,1,3), each=50))
plot_ly(cars2,
        x = ~speed,
        y = ~dist,
        frame = ~frame,
        type = 'scatter',
        mode = 'markers',
        showlegend = F,
        marker=list(color="firebrick")) %>%
  layout(title="Stopping Distance of 1920's Vehicles\n (cars data set)",
         xaxis=list(title="Vehicles Speed in mph (speed)"),
         yaxis=list(title="Stopping Distance in Feet (dist)")) %>%
  add_segments(x = 0, xend = 25, y = mean(cars$dist), yend = mean(cars$dist), line=list(color="gray", dash="dash", width=1), inherit=FALSE, name=TeX("\\bar{Y}"), showlegend=T) %>%
  add_segments(x = 0, xend = 25, y = sum(coef(cars.lm)*c(1,0)), yend = sum(coef(cars.lm)*c(1,25)), line=list(color="darkgray", width=1), inherit=FALSE, name=TeX("\\hat{Y}"), showlegend=T) %>%
  config(mathjax = 'cdn') %>%
  animation_opts(frame=2000, transition=1000, redraw=T)

# Friday
library(tidyverse)
View(diamonds)
plot(price ~ carat, data=diamonds)

diamonds.lm <- lm(price ~ carat, data=diamonds)
summary(diamonds.lm)

abline(diamonds.lm, col="green", lwd=2)

plot(dist ~ speed, data=cars)
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
abline(cars.lm)
points(dist ~ speed, data=cars[23,], col="orange", pch=16)
```

##### Quizzes ri

```{r eval=FALSE}
# Mobius
# wt
mtwt.lm <- lm(mpg~wt, mtcars)
summary(mtwt.lm)

ggplot(mtcars, aes(y=mpg, x=wt)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtwt.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtwt.ssr <- sum( (mtwt.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtwt.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(wtr2 <-mtwt.ssr/mtwt.ssto)


# r is square root of r^2
sqrt(wtr2)

predict(mtwt.lm, data.frame(wt=3*365))

par(mfrow=c(1,3))
plot(mtwt.lm, which = 1:2)
plot(mtwt.lm$residuals)


# cyl
View(mtcars)
mtcyl.lm <- lm(mpg~cyl, mtcars)
summary(mtcyl.lm)
mean(mtcars$mpg)

ggplot(mtcars, aes(y=mpg, x=cyl)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtcyl.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtcyl.ssr <- sum( (mtcyl.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtcyl.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(cylr2 <-mtcyl.ssr/mtcyl.ssto)


# r is square root of r^2
sqrt(cylr2)

predict(mtcyl.lm, data.frame(cyl=3*365))

par(mfrow=c(1,3))
plot(mtcyl.lm, which = 1:2)
plot(mtcyl.lm$residuals)


# Assessment: Residuals, Sums of Squares, and R squared
# 1) A regression was performed for a sample of n = 5 data points.
# The y-values of the regression are: 3.78, 6.08, 6.65, 9.25, and 9.92.
# The residuals from the regression are: -0.266, 0.489, -0.486, 0.569, and -0.306.
# What is the R-squared value for this regression?
y <- c(3.78, 6.08, 6.65, 9.25, 9.92)

SSTO <- sum( (y - mean(y))^2 )
#SSTO = 24.83372

res <- c(-0.266, 0.489, -0.486, 0.569, -0.306)
SSE <- sum(res^2)
#SSE = 0.96347

R2 = 1 - SSE/SSTO
#R2 = 0.9612032

# 2) This data can be used to show that the displacement of the engine (disp)
# is positively correlated with the weight of the vehicle (wt).
# Apparently, this means disp is the Y and wt is the x
# Don't use mylm unless you've cleared everything or use something descriptive like clm for cars mylm
mt.lm <- lm(disp ~ wt, data=mtcars)
summary(mt.lm)
# Multiple R-squared: 0.7885, Adjusted R-squared: 0.7815  <- (multiple R-squared is R^2)

# 3)
# A certain statistics teacher at BYU-Idaho drives a 2001 Nissan Sentra that weighs
# approximately 2,700 lbs and currently gets only 21 mpg.
# Based on the regression you performed, how many mpg above or below average is this vehicle?
#   Perform a regression of mpg means mpg is the Y and wt is the x
mt.lm <- lm(mpg ~ wt, data=mtcars) #perform the regression
plot(mpg ~ wt, data=mtcars) #draw the regression (not needed, but nice)
abline(mt.lm) #add the regression line (not needed, but nice)
points(2.7, 21, pch=16, col="skyblue") #add the Nissan Sentra value (not needed, but nice)
predict(mt.lm, data.frame(wt=2.7)) #get predicted value for Nissan Sentra
1
22.85505
lines(c(2.7, 2.7), c(21, 22.85505), col="skyblue") #add residual line (not needed, but nice)
myresidual <- 21 - 22.85505 #calculate difference between Y and Y-hat

> myresidual
[1] -1.85505
```
<br>



### 03 Diagnostics

par(mfrow=c(1,3))
plot(mylm, which = 1:2)
plot(mylm$residuals)

##### Class Code Diag

##### Quizzez Diag

<br>

## Code notes

##### Generic
Ctrl + L to clear console
Getwd()<br>
Glimpse() to see the data and it's types (good for when ? Doesn't work)<br>
In rmd ctr shft c to comment/uncomment<br>
Echo = false means the code won't show just output<br>
Double question marks tells you where<br>
Don't put install in the script cause it'll do it every time<br>
Say no to saving workspace instead save a script with everything.<br>
Shortcut for pipe is ctrl + shift + m<br>

##### Graphics
Notebook add links to rmd files (can also put line number)<br>
P + theme(text=element_text(size24))

##### Problems
Mismatched directories can sometimes cause you problems when you knit.<br> 
Session>Set Working Directory to Source file location<br>
NAs are contagious they will permeate your data, pass na.rm true to remove the nas<br>
Session > restart R because he overwrote his data<br>
Clicking on broom cleans out all the data<br>

##### Reminders
Don't do anything to the original data file make sure to assign it to a new name!!!<br>
Filter rows/select columns<br>
Colnames() to see the column names<br>
Tibble() see code for names and first few rows<br>
Easier to do test questions in base R<br>
Filter into a new data set before summarizing<br>

<a id=" Code Notes"></a>



