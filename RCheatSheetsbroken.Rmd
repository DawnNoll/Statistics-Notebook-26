---
title: "R Cheat Sheets & Notes"
output: html_document
params:
  run_code: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

[Test Tips], [Code Notes] <br>
Class Notes: [Week 1] , [Week 2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14]<br>
Useful Tidbits: <a href="Notes">Class Notes</a><br>
[Intermediate Stats Notes](IntermediateStatsNotes.html)

### Cheat Sheets

* [R Color Guide](file:///C:/Users/rizen/OneDrive/Documents/BYUI/DataScience Certificate/INT STAT/Statistics-Notebook-master/RColorGuide.html)

* [R Base Graphics Cheat Sheet](http://www.joyce-robbins.com/wp-content/uploads/2016/04/BaseGraphicsCheatsheet.pdf)

* [R Base Commands Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)

* [R Markdown Cheat Sheet](https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf)

* [ggplot2 Cheat Sheet](https://rstudio.com/wp-content/uploads/2016/11/ggplot2-cheatsheet-2.1.pdf)

* [Keyboard Shortcuts](https://support.rstudio.com/hc/en-us/articles/200711853-Keyboard-Shortcuts)<br/>

#### Save week 2 definitionsn

### Test Tips

Read output carefully so you understand what it's saying! For quartemile time (qsec) a lower number is  better.<br>
Read carefully so you don't miss info in the questions.<br>
Make sure to check whether the answer is positive or negative.<br>
Make sure you put a zero before the decimal in Canvas.<br>

### Analysis code index
#### Weather Analysis 


#### Theory 1



### Code Notes

#### Summary

Std. Error = estimated standard deviation or standard error or residual standard error
Multiple R-squared (R2) = Multiple R-squared

#### Generic
Ctrl + L to clear console
Getwd()<br>
Glimpse() to see the data and it's types (good for when ? Doesn't work)<br>
In rmd ctr shft c to comment/uncomment<br>
Echo = false means the code won't show just output<br>
Double question marks tells you where<br>
Don't put install in the script cause it'll do it every time<br>
Say no to saving workspace instead save a script with everything.<br>
Shortcut for pipe is ctrl + shift + m<br>

#### Graphics
Notebook add links to rmd files (can also put line number)<br>
P + theme(text=element_text(size24))

#### Problems
Mismatched directories can sometimes cause you problems when you knit.<br> 
Session>Set Working Directory to Source file location<br>
NAs are contagious they will permeate your data, pass na.rm true to remove the nas<br>
Session > restart R because he overwrote his data<br>
Clicking on broom cleans out all the data<br>

#### R Markdown

#### Reminders
Don't do anything to the original data file make sure to assign it to a new name!!!<br>
Filter rows/select columns<br>
Colnames() to see the column names<br>
Tibble() see code for names and first few rows<br>
Easier to do test questions in base R<br>
Filter into a new data set before summarizing<br>

<a id="Notes"></a>

### Class Notes

#### 1, Linear Regression

B0 B1 and sigma^2 are for the group
the i's are for the individual
since we can't know the errors we estimated them with the residual

Three main elements to the mathematical model of regression.

*  The true, unobservable line of the estimated value of the population (regression relation), provides the average Y-value. The expected value of Y is equal to Beta0 + Beta1*Xi $ \underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\ $
*  The dots (regression plus error term), $Y $\epsilon_i$ allows each individual $i$ to deviate from the line. $Y_i = \underbrace{\beta_0 + \beta_1 X_i}_{E\{Y_i\}} + \underbrace{\epsilon_i}_\text{error term} \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)$
*  The estimated line (the line we get from the sample data) $\hat{Y}_i$ is the estimated equation and is interpreted as the estimated average $Y$ for any $X$ $\underbrace{\hat{Y}_i}_{\substack{\text{estimated mean} \\ \text{y-value}}} = \underbrace{b_0 + b_1 X_i}_\text{estimated regression equation}$

$E{Y}$ true mean Y
$Y_i$ the dots (add error term to above), the regression model
$\hat{Y}_i = b_0 + b_1 X_i$, fitted line, lmObject$fitted.values
$b_0$ Estimated intercept, 	b_0 <- mean(Y) - b_1*mean(X)
$b_1$ Estimated slope, b_1 <- sum( X*(Y - mean(Y)) ) / sum( (X - mean(X))^2 )
$r_i$ residual-eye, distance of dot from line, lmObject$residuals
$\sigma^2$ variance of $\epsilon_i$

Standard error = estimated standard deviation = the residual standard error in summary
An increase of 1,000 lbs in the weight of a vehicle results in a 5.34 mpg decrease in the average gas mileage of such vehicles.

The interpretation of β1 is the amount of increase (or decrease) in the average y-value, denoted E{Y}, per unit change in X. It is often misunderstood to be the “average change in y” or just “the change in y” but it is more correctly referred to as the “change in the average y”.


##### Class Code

```{r, eval=FALSE}
library(tidyverse)
View(airquality)
?airquality

# Basics
library(mosaic) #favstats
favstats(airquality$Temp)
mean(airquality$Temp)
sd(airquality$Temp)

# Histograms
hist(airquality$Temp)

ggplot(airquality, aes(x=Temp)) +
geom_histogram(binwidth=5, fill="skyblue", color="skyblue4") +
labs(title="Maximum daily...", subtitle = "May to September 1973", 
     x="Temperature in Degrees F", y="Number of Days in Temperature Range")

# Boxplot 
boxplot(Temp ~ Month, data=airquality)

ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot()

# Scatterplot
mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)
predict(mylm, data.frame(Wind=19))

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x)

favstats(Temp ~ Month, data=airquality)
ggplot(airquality, aes(x=as.factor(Month), y=Temp)) + 
  geom_boxplot(fill="skyblue", color="skyblue3") + 
  labs(title="Maximum daily temperature in degrees Fahrenheit at La Guardia Airport, NY, USA",
       subtitle="May to September 1973",
       y="Temperature in degrees F", x="Month of the Year")


library(tidyverse)

ggplot(airquality, aes(x=Wind, y=Temp)) + 
  geom_point(fill="skyblue", pch=21, color="skyblue4") + 
  geom_smooth(method="lm", se=F, formula=y~x, color="skyblue4") + 
  labs(title="Max...", subtitle="May...", 
       y="Temp...", x="Average...")

mylm <- lm(Temp ~ Wind, data=airquality)
summary(mylm)






set.seed(101) #Allows us to always get the same "random" sample
#Change to a new number to get a new sample

n <- 153 #set the sample size

X_i <- runif(n, 15, 45) #Gives n random values from a uniform distribution between 15 to 45.

beta0 <- 89 #Our choice for the y-intercept. 

beta1 <- -1.25 #Our choice for the slope. 

sigma <- 1 #Our choice for the std. deviation of the error terms.

epsilon_i <- rnorm(n, 0, sigma) #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

Y_i <- beta0 + beta1*X_i + epsilon_i #Create Y using the normal error regression model

fabData <- data.frame(y=Y_i, x=X_i) #Store the data as data

View(fabData) 

#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.

fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

summary(fab.lm) #Summarize your model. 

plot(y ~ x, data=fabData, ylim=c(20,100)) #Plot the data.

abline(fab.lm) #Add the estimated regression line to your plot.

# Now for something you can't do in real life... but since we created the data...

abline(beta0, beta1, lty=2) #Add the true regression line to your plot using a dashed line (lty=2). 

legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n") #Add a legend to your plot specifying which line is which.
```



##### Quizzes

```{r eval=FALSE}

# Assessment Quiz
dav.lm <- lm(weight ~ height, data=subset(Davis, sex=="M"))
predict(dav.lm, data.frame(height=180))

# Perform a regression using this data that explains the average number of murder arrests in cities (per 100,000 in 1973) using the number of assault arrests (per 100,000) in a city.

# Select the answer that provides the correct estimate of B1 in the formula:for the USArrests regression described above.
lm(formula = Murder ~ Assault, data = USArrests)

# Which of the following statements is a correct statement about the graphic shown below?

# Note: the "Line of Equality" shows where the line would need to be if the reported heights were equal (on average) to the actual measured heights. Dots that fall on this line show men that knew their actual height before they were officially measure

# The average actual height gets closer to the reported height for taller men, while shorter men seem more likely to under-report their height, on average.

par(mfrow=c(1,3))
plot(mylm, which = 1:2)
plot(mylm$residuals)


```



#### 2. ri, Sum of Squares, and R-squared


##### Terms

###### Mean Measure of Center

add all the data values up and divide by the total number of values<br>
mean(object) or mean(object, na.rm=TRUE)<br>
$$\bar{x} = \frac{\sum_{i=1}^n x_i}{n}$$
<br>

###### Variance Measure of Spread

Seldom used because it's diffucult to interpret due to squared units (use sd instead, notes $s$)<br>
var(object)<br>
<br>

$$s^2 = \frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}$$

Variance is squared units, sd is in original units<br>
Variance is an average of the sum of squares<br>

*  the values being squared in the numerator are the deviations of each data point from mean
*  Var is large when very spread from mean
*  Var is small when data tightly clustered around mean
*  Var is 0 when data equal to mean
*  Var never negative
<br>

###### Standard Deviation Measure of Spread

Sometimes called the RMSE (root mean squared error)<br>
The denominator is called the degrees of freedom ($n$-1)<br>
Never negative only 0 if all values are the same.
$$s = \sqrt{\frac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}}$$

We will denote a residual for individual $i$ by $r_i$,<br>

$$r_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}} - \underbrace{\hat{Y}_i}_{\substack{\text{Predicted} \\ \text{Y-value}}} \quad \text{(residual)}
$$

###### Residual 


The residual $r_i$ estimates the true error for individual $i$, $\epsilon_i$ <br>

$$\epsilon_i = \underbrace{Y_i}_{\substack{\text{Observed} \\ \text{Y-value}}} - \underbrace{E\{Y_i\}}_{\substack{\text{True Mean} \\ \text{Y-value}}} \quad \text{(error)}
$$
Keep in mind the idea that the errors $\epsilon_i$ “created” the data and that the residuals $r_i$ are computed after using the data to “re-create” the line.<br>
The residual tells how far you are from the line.<br>

Residuals have many uses in regression analysis. They allow us to

*  diagnose the regression assumptions (Assumptions)
*  estimate the regression relation (Estimating the Model Parameters)
*  estimate the variance of the error terms (Estimating the Model Variance)
*  and assess the fit of the regression relation (Assessing the Fit of a Regression)<br>


[Regression Applet](https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html) with sliders



SSE SSR SSTO R2 r(correlation)

Multiple R-squared is R^2

If the dots are on the line the correlation coefficient moves toward 1 and the sum of squared residuals goes to 0.
Correlation describes the strength and direction of association between two quantative variables, between -1 and 1. 0 means no association

###### Correlation Measure of Association

Measures the strength and direction between to quantative vars<br>
The square root of SSR divided by SSTO gives the value of the correlation.<br>
Greater insight in using the square of corr -> $R^2$
cor(object1,object2)<br>

In fact, the correlation squared is equal to SSR/SSTO ($R^2$)


$$r = \frac{\textstyle\sum\left(\frac{x-\bar{x}}{s_x}\right)\left(\frac{y-\bar{y}}{s_y}\right)}{n-1}$$
###### SSE Sum of Squared Errors (actually residuals)

Measures how much the residuals deviate from the regression line<br>
SSE = SSTO-SSR<br>
$$\text{SSE} = \sum_{i=1}^n \left(Y_i - \hat{Y}_i\right)^2$$
<br>
SSE is the unexplained variability.
sum( lmObject$res^2 )
```{r, eval=false}
sum(cars.lm$res^2 )
```
<br>

###### SSR Sum of Squares Regression
Measures how much the regression line deviates from the average y-value.<br>
A good regression should decrease the var, so SSE should be less than SSTO.<br>
SSR = SSTO - SSE<br>
$$\text{SSR} = \sum_{i=1}^n \left(\hat{Y}_i - \bar{Y}\right)^2$$
<br>
SSR is the explained variablility
sum( (lmObject$fit - mean(YourData$Y))^2 )
```{r,eval=false}
cars.ssr <- sum( (cars.lm$fit - mean(cars$dist))^2 )
```

<br>

###### SSTO Total Sum of Squares

Measures how much the y-values deviate from the average y-value (the total amount of variability in your data).<br>
SSTO = SSE + SSR<br>
$$\text{SSTO} = \sum_{i=1}^n \left(Y_i - \bar{Y}\right)^2$$
SSTO
sum( (YourData$Y - mean(YourData$Y))^2 )
```{r, eval=false}
cars.ssto <- sum( (cars$dist - mean(cars$dist))^2 )
```

If SSE is large (close to SSTO) then SSR is small (close to zero) and visa versa. The following three graphics demonstrate how this works.<br>
<br>
SSE small SSR large -> excellent fit<br>
SSE medium SSR medium -> good fit<br>
SSE large SSR small -> poor fit<br>
<br>

###### R-Squared  

$$\underbrace{R^2 = \frac{SSR}{SSTO} = 1 - \frac{SSE}{SSTO}}_\text{Interpretation: Proportion of variation in Y explained by the regression.}$$
If this is high,we're getting more explanation. If it's close to 0 then it's close to the average, so it's not telling us anything.The smallest R2 can be is zero, and the largest it can be is 1. This is because SSR must be between 0 and SSTO, inclusive.<br>
Any time you square a decimal it gets smaller, so R^2 is stricter on the correlation.<br>
R2 is the ratio between explanation and total SSR/SSTO
```{r, eval=false}
cars.ssr/cars.ssto
```

Note: r (correlation is the square root of $R^2$)

```{r, eval=false}
sqrt(cars.ssr/cars.ssto)
```

The only way your line could be worse than ybar is if it moved away from the data so it's not possible because Yhat is the best fit (is in the middle).
Why does dividing SSR/SSTO give a good? We want $\hat{Y}$ to be far from $\hat{Y}$--we're getting more explanation. If it's close to 0 then it's close to the average, so it's not telling us anything.
Proportion of variation in Y explained by the regression.

##### Class Code



```{r eval=FALSE}
# Monday
cars2 <- cars[sample(1:50, 10), ]
View(cars2)

library(tidyverse)

carslm <- lm(dist ~ speed, data=cars2)
summary(carslm)
predict(carslm, data.frame(speed=17.5))

# add line segments
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_segment(aes(x=17.5, xend=17.5, y=45.297, yend=50))


# add squares
ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_point(aes(x=17.5, y=45.297), color="red", cex=2) + 
  geom_point(aes(x=17.5, y=50), color="blue", cex=2) + 
  geom_rect(aes(xmin=17.5, xmax=17.5+.65, ymin=45.297, ymax=50), alpha=0.1) + 
  geom_rect(aes(xmin=speed, xmax=speed+carslm$residuals*.15, ymin=dist, ymax=carslm$fitted.values), alpha=0.1, fill="blue")



install.packages("plotly")
library(plotly)

x = c(5, 15, 2, 29, 35, 24, 25, 39) 
sum(x)
sum( (1:6)^2 ) 
sum(x^2)
mn <- mean(x)
var(x)
dist <-(x-mn)
dist
sum(dist^2)/7
mean(cars$dist)
sum((1:3)^2)
```

The $i$ in $x_i$ stands for individual
$\bar{x_i}$ is the deviation
Negative areas don't exist. Cannot use the abs value instead of squaring because abs values aren't differentiatable.
$n-1$ (degrees of freedom)--the very last person in has no freedom to select where they sit in the classroom.
If we try to use the same thing twice we get penalized -1.
When add up the squares and divide by n-1 we get the average(ish) of square or mean square divide sum of squares by degrees of freedom
$\hat{Y}$ is the regression line. $\bar{Y}$ is the average flat line. $R^2$ the amount of h variation in Y that we can explain with the regression (our regression has reduced the variation around $\hat{Y}$).

```{r, eval=false}
# Wednesday
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
cars2 <- cbind(rbind(cars, 
                     cbind(speed=rep(0,50), dist=cars$dist),
                     cbind(speed=cars$speed, dist=cars.lm$fitted.values)),
               frame = rep(c(2,1,3), each=50))
plot_ly(cars2,
        x = ~speed,
        y = ~dist,
        frame = ~frame,
        type = 'scatter',
        mode = 'markers',
        showlegend = F,
        marker=list(color="firebrick")) %>%
  layout(title="Stopping Distance of 1920's Vehicles\n (cars data set)",
         xaxis=list(title="Vehicles Speed in mph (speed)"),
         yaxis=list(title="Stopping Distance in Feet (dist)")) %>%
  add_segments(x = 0, xend = 25, y = mean(cars$dist), yend = mean(cars$dist), line=list(color="gray", dash="dash", width=1), inherit=FALSE, name=TeX("\\bar{Y}"), showlegend=T) %>%
  add_segments(x = 0, xend = 25, y = sum(coef(cars.lm)*c(1,0)), yend = sum(coef(cars.lm)*c(1,25)), line=list(color="darkgray", width=1), inherit=FALSE, name=TeX("\\hat{Y}"), showlegend=T) %>%
  config(mathjax = 'cdn') %>%
  animation_opts(frame=2000, transition=1000, redraw=T)

library(tidyverse)
View(diamonds)
plot(price ~ carat, data=diamonds)

diamonds.lm <- lm(price ~ carat, data=diamonds)
summary(diamonds.lm)

abline(diamonds.lm, col="green", lwd=2)





plot(dist ~ speed, data=cars)
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
abline(cars.lm)
points(dist ~ speed, data=cars[23,], col="orange", pch=16)

View(cars)
cars[23, ]
cars.lm$residuals[23]
cars$dist[23] - cars.lm$fitted.values[23] #Y_i - Yhat_i
```



##### Quizzes


```{r eval=FALSE}
# Mobius
# wt
mtwt.lm <- lm(mpg~wt, mtcars)
summary(mtwt.lm)

ggplot(mtcars, aes(y=mpg, x=wt)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtwt.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtwt.ssr <- sum( (mtwt.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtwt.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(wtr2 <-mtwt.ssr/mtwt.ssto)


# r is square root of r^2
sqrt(wtr2)

predict(mtwt.lm, data.frame(wt=3*365))

par(mfrow=c(1,3))
plot(mtwt.lm, which = 1:2)
plot(mtwt.lm$residuals)


# cyl
View(mtcars)
mtcyl.lm <- lm(mpg~cyl, mtcars)
summary(mtcyl.lm)
mean(mtcars$mpg)

ggplot(mtcars, aes(y=mpg, x=cyl)) +
  geom_point() +
  geom_smooth(method="lm", se=F, forumula=y~x)

# SSE is the unexplained variability
# sum( lmObject$res^2 )
sum(mtcyl.lm$res^2 )

# SSR is the explained variablility
# sum( (lmObject$fit - mean(YourData$Y))^2 )
(mtcyl.ssr <- sum( (mtcyl.lm$fit - mean(mtcars$mpg))^2 ))

# SSTO
#sum( (YourData$Y - mean(YourData$Y))^2 )
(mtcyl.ssto <- sum( (mtcars$mpg - mean(mtcars$mpg))^2 ))


# R2 is the ration between explanation and total SSR/SSTO
(cylr2 <-mtcyl.ssr/mtcyl.ssto)


# r is square root of r^2
sqrt(cylr2)

predict(mtcyl.lm, data.frame(cyl=3*365))

par(mfrow=c(1,3))
plot(mtcyl.lm, which = 1:2)
plot(mtcyl.lm$residuals)


# Assessment: Residuals, Sums of Squares, and R squared
# A regression was performed for a sample of n = 5 data points.
# The y-values of the regression are: 3.78, 6.08, 6.65, 9.25, and 9.92.
# The residuals from the regression are: -0.266, 0.489, -0.486, 0.569, and -0.306.
# What is the R-squared value for this regression?
y <- c(3.78, 6.08, 6.65, 9.25, 9.92)

SSTO <- sum( (y - mean(y))^2 )
#SSTO = 24.83372

res <- c(-0.266, 0.489, -0.486, 0.569, -0.306)
SSE <- sum(res^2)
#SSE = 0.96347

R2 = 1 - SSE/SSTO
#R2 = 0.9612032


mt.lm <- lm(mpg ~ wt, data=mtcars) #perform the regression
plot(mpg ~ wt, data=mtcars) #draw the regression (not needed, but nice)
abline(mt.lm) #add the regression line (not needed, but nice)
points(2.7, 21, pch=16, col="skyblue") #add the Nissan Sentra value (not needed, but nice)
predict(mt.lm, data.frame(wt=2.7)) #get predicted value for Nissan Sentra
1
22.85505
lines(c(2.7, 2.7), c(21, 22.85505), col="skyblue") #add residual line (not needed, but nice)
myresidual <- 21 - 22.85505 #calculate difference between Y and Y-hat

myresidual
-1.85505

```

<br>

#### 3. Diagnosing the Model and Model Transformations

Residual Plots & Regression Assumptions & Transformations

*  residuals vs. fitted-values (which=1), most important
*  Q-Q Plot of Residuals (which=2), 2nd most important
*  Residuals vs Order (lm$residals)
Five Assumptions:

1)  The regression relation between Y and X is linear. res v fit (red line straight) If bent results are meaningless--it affects everything. plot(mylm, which=1)
2)  The error terms are normally distributed with E{ϵi}=0. QQ checks normality -> unwise to put too much trust in the residual standard error as an estimate of the standard deviation σ when not normal dist. plot(mylm, which=2)
3)  The variance of the error terms is constant over all X values. Distance constant on res v fit--the residual standard error however should not be considered to be meaningful as it will be too large on one end of the regression and too small on the other end. plot(mylm, which=1)
4)  The X values can be considered fixed and measured without error.
5)  The error terms are independent. res v order (no pattern)--While the slope and intercept are often still meaningful when the independence assumption is violated, the residual standard error could be unnecessarily large in this case. plot(mylm$residuals, ylab="Residuals")
$$Y_i = \underbrace{\beta_0 + \beta_1 \overbrace{X_i}^\text{#4}}_{\text{#1}} + \epsilon_i \quad \text{where} \ \overbrace{\epsilon_i \sim}^\text{#5} \overbrace{N(0}^\text{#2}, \overbrace{\sigma^2}^\text{#3})$$


How does plot change when we look at residuals vs fitted-values Which is the y
How check assumption 4 Does it make sense th.n having x fixed is workes
Explain the difficulties that arise when there is an outlier present. (how do outliers skew) biases the slope


##### Class Code

```{r eval=FALSE}
lm.mt <- lm(mpg ~ qsec, data=mtcars)
plot(mpg ~ qsec, data=mtcars)
abline(lm.mt)

plot(lm.mt, which=1)




plot(drat ~ wt, data=mtcars)
lm2 <- lm(drat ~ wt, data=mtcars)
abline(lm2)

plot(lm2, which=1)


plot(height ~ age, data=Loblolly)
lmlob <- lm(height ~ age, data=Loblolly)
abline(lmlob)

plot(lmlob, which=1)

summary(lmlob)

plot(circumference ~ age, data=Orange)





boxplot(islands, col="forestgreen")
boxplot(log(islands), col="forestgreen")



library(mosaicData)
View(Utilities)
?Utilities



gas.lm <- lm(gasbill ~ temp, data=Utilities)
plot(gasbill ~ temp, data=Utilities)
abline(gas.lm)
# res v fitted
plot(gas.lm, which=1)


gas.lm.log <- lm(log(gasbill) ~ temp, data=Utilities)
b <- coef(gas.lm.log)

plot(log(gasbill) ~ temp, data=Utilities)
abline(gas.lm.log)


plot(gas.lm.log, which=1)
summary(gas.lm.log)


plot(gasbill ~ temp, data=Utilities)
curve(exp(6.031885 - 0.041435*x), add=TRUE)

plot(gasbill ~ temp, data=Utilities)
abline(gas.lm)
curve(exp(b[1] + b[2]*x), add=TRUE, col="skyblue")

# Graphing Transformations
ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  stat_function(fun=function(x) ())
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( )

# Bro Saunders code
  lm.log <- lm(log(circumference) ~ age, data=Orange)
b.log <- coef(lm.log)
b.log[1]
b.log[2]

lm.sqrt <- lm(sqrt(circumference) ~ age, data=Orange)
b.sqrt <- coef(lm.sqrt)

lm.1oY <- lm(1/circumference ~ age, data=Orange)
b.1oY <- coef(lm.1oY)

lm.y <- lm(circumference ~ age, data=Orange)
b.y <- coef(lm.y)

lm.y2 <- lm(circumference^2 ~ age, data=Orange)
b.y2 <- coef(lm.y2)

lm.1oY2 <- lm(1/circumference^2 ~ age, data=Orange)
b.1oY2 <- coef(lm.1oY2)


# Base Graphic


plot(circumference ~ age, data=Orange, pch=16, col="orangered", main="Growth of Orange Trees", xlab="Age of Tree in Days", ylab="Circumference of Tree (mm)")

curve( exp(b.log[1] + b.log[2]*x), add=TRUE, col="red")
curve( (b.sqrt[1] + b.sqrt[2]*x)^2, add=TRUE, col="blue")
curve( 1/(b.1oY[1] + b.1oY[2]*x), add=TRUE, col="green")
curve( b.y[1] + b.y[2]*x, add=TRUE, col="gray")
curve( sqrt(b.y2[1] + b.y2[2]*x), add=TRUE, col="orange")
curve( 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), add=TRUE, col="forestgreen")

# ggplot Graphic

library(tidyverse)

ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  stat_function(fun=function(x) exp(b.log[1] + b.log[2]*x), aes(color="log(Y)")) + 
  stat_function(fun=function(x) (b.sqrt[1] + b.sqrt[2]*x)^2, aes(color="sqrt(Y)")) + 
  stat_function(fun=function(x) 1/(b.1oY[1] + b.1oY[2]*x), aes(color="1/Y")) + 
  stat_function(fun=function(x) b.y[1] + b.y[2]*x, aes(color="Y")) + 
  stat_function(fun=function(x) sqrt(b.y2[1] + b.y2[2]*x), aes(color="Y^2")) + 
  stat_function(fun=function(x) 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), aes(color="1/Y^2")) + 
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( ) 

library(car)
boxCox(lm.y)


YoungOrange <- filter(Orange, age < 1200)


lm.log <- lm(log(circumference) ~ age, data=YoungOrange)
b.log <- coef(lm.log)

lm.sqrt <- lm(sqrt(circumference) ~ age, data=YoungOrange)
b.sqrt <- coef(lm.sqrt)

lm.1oY <- lm(1/circumference ~ age, data=YoungOrange)
b.1oY <- coef(lm.1oY)

lm.y <- lm(circumference ~ age, data=YoungOrange)
b.y <- coef(lm.y)

lm.y2 <- lm(circumference^2 ~ age, data=YoungOrange)
b.y2 <- coef(lm.y2)

lm.1oY2 <- lm(1/circumference^2 ~ age, data=YoungOrange)
b.1oY2 <- coef(lm.1oY2)



ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  geom_vline(aes(xintercept=1200)) + 
  stat_function(fun=function(x) exp(b.log[1] + b.log[2]*x), aes(color="log(Y)")) + 
  stat_function(fun=function(x) (b.sqrt[1] + b.sqrt[2]*x)^2, aes(color="sqrt(Y)")) + 
#  stat_function(fun=function(x) 1/(b.1oY[1] + b.1oY[2]*x), aes(color="1/Y")) + 
  stat_function(fun=function(x) b.y[1] + b.y[2]*x, aes(color="Y")) + 
  stat_function(fun=function(x) sqrt(b.y2[1] + b.y2[2]*x), aes(color="Y^2")) + 
#  stat_function(fun=function(x) 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), aes(color="1/Y^2")) + 
  ylim(c(0,300)) + 
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( ) 
```





##### Quizzes

Mean Squared Error
$$\begin{equation}
  s^2 = MSE = \frac{SSE}{n-2} = \frac{\sum(Y_i-\widehat{Y}_i)^2}{n-2} = \frac{\sum r_i^2}{n-2}
\end{equation}$$

``` {r, eval=FALSE}
View(Orange)
o.lm <- lm(circumference ~ age, data=Orange)
summary(o.lm)


ggplot(data=Orange, mapping=aes(y=circumference, x=age)) + geom_point(color = "orange", pch=16) +
geom_smooth(method="lm", formula=y~x, se=F) +
stat_function(fun=function(x) (b[1] + b.[2]*x)^2, aes(color="sqrt(Y)")) + 
  theme_minimal()

# sqrt(MSE)
sqrt(sum(o.lm$residuals^2)/(35-2))

plot(o.lm, which=1:2)
plot(o.lm$residuals)
# the variance is not constant, minor issues with linearity, minor issues with normality

library(car)
boxCox(o.lm)
o.sqrt.lm <- lm(sqrt(circumference) ~ age, data=Orange)
summary(o.sqrt.lm)
b <- coef(o.sqrt.lm)
t.lm <- lm((b[0] + b[1]*age)^2 ~ age, data=Orange)

ggplot(data=Orange, mapping=aes(y=sqrt(circumference), x=age)) + geom_point(color = "orange", pch=16) +
geom_smooth(method="lm", formula=y~x, se=F) +
  theme_minimal()

plot(o.sqrt.lm, which=1:2)
plot(o.sqrt.lm$residuals)

plot(circumference ~ age, data=Orange)
abline(o.lm)
curve((b[1] + b[2]*x)^2, add=TRUE)

ggplot(data=Orange, mapping=aes(y=circumference, x=age)) + geom_point(color = "orange", pch=16) +
geom_smooth(method="lm", formula=y~x, se=F) +
  
  theme_minimal()

o.pred <- predict(o.sqrt.lm, data.frame(age=500))
o.pred^2  # use predict on the transformed model and then reverse the answer
```



<br>

#### 4. Hypothesis Tests for Model Parameters

From Intermediate Stats: 
1. State the null and alternative hypotheses.<br>
2. Determine p-value based on test statistic<br>
3 decision regrading Ho<br>
4 State the conclusion (in context). What do you say about Ho<br>
  a. If reject Ho We have sufficient evidence to (Ha in English)<br>
  b. If don't reject Ho- We have insufficient evedience to say that "Ha in English"<br>
Remember Art (Alpha/Type 1 Reject True Ho) and BFF (Beta/Type II Fail to reject False Ho)

                                Ho is True        Ha is True<br>
conclusion: Don't Reject Ho      Correct        Type II error<br>
Reject Ho                       Type I Error        Correct<br>

Probabliity of Type 1 error = alpha (convicting an innocent man of a crime)<br>
Probability of Type II Error = Beta (letting a guilty man go free)<br>
Null hypothesis in jury is innocence so we either reject nul (guilty) or fail to reject null (not guilty), but we don't accept the null (innocence)<br>

 68-95-99.7 rule, with a normal distribution we can assume that within 2 standard deviations above and below the mean lies 95% of the data.
	
Some of the most common approaches to making inference about $\mu$ utilize a test statistic that follows a t distribution.
	
##### Class Code

faithful[-5,] remove outlier

$H_0:\beta_1 = 14$
find t: slope estimate-hyp (14)/std.error

Standard error (estimated sd) = residual standard error

$$t = \frac{b_0 - \overbrace{0}^\text{a number}}{s_{b_0}}$$
pt(-abs(tvalue), degrees of freedom) for left tailed or *2 for both

$$MSE = \frac{\sum_{i=1}^n (Y_i - \hat{Y}_i)^2}{n-2} = \frac{SSE}{n-2}$$
True Value of standard error of $b_1$<br>
$$\sigma^2_{b_1} = \frac{\sigma^2}{\sum_{i=1}^n(X_i - \bar{X})^2}$$ 

Estimated value of standard error of $b_1$<br>
$$s^2_{b_1} = \frac{MSE}{\sum_{i=1}^n(X_i - \bar{X})^2}$$ 
<br>
<br>
True Value of standard error of $b_2$<br>
$$\sigma^2_{b_0} = \sigma^2 \left[\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\right]$$

Estimated value of standard error of $b_2$<br>
$$s^2_{b_0} = MSE\left[\frac{1}{n} + \frac{\bar{X}^2}{\sum_{i=1}^n(X_i - \bar{X})^2}\right]$$
The first term is variability in the data and the second is the variability on the Y.

```{r eval=FALSE}
confint(mylm)
```


```{r eval=FALSE}
# Sampling Distributins of Model Parameters


# Hypothesis Tests for Model Parameters

## Simulation to Show relationship between Standard Errors

##-----------------------------------------------
## Edit anything in this area... 

n <- 100 #sample size
Xstart <- 30 #lower-bound for x-axis
Xstop <- 100 #upper-bound for x-axis

beta_0 <- 2 #choice of true y-intercept
beta_1 <- 3.5 #choice of true slope
sigma <- 13.8 #choice of st. deviation of error terms

## End of Editable area.
##-----------------------------------------------
```

**True Model**

$$
  Y_i = \overbrace{\beta_0}^{`r beta_0`} + \overbrace{\beta_1}^{`r beta_1`} X_i + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \overbrace{\sigma^2}^{\sigma=`r sigma`})
$$

```{r, fig.height=8, fig.width=8, eval=FALSE}
X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) #Create X #n must be even
N <- 5000 #number of times to pull a random sample
storage_b0 <- storage_b1 <- storage_rmse <- rep(NA, N)
for (i in 1:N){
  Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
# B0 + B1 is fixed (deterministic), rnorm is random (stochastic) as sigma is allowed to increase, we see more fuzz around the data/line and the harder it is to know 
# the last r norm is the sigma, we're telling the data what law to obey
  # could do runif(n, 30, 100) random uniform, the numbers are different each time and the lowest is may be as low or high as the lowest and highest value
# assumption x is fixed and without error
  
# want to look at the distribution of   get a sample, get stat, store stat, get a new sample (scatterplot), then throw it back after you've captured the line
  mylm <- lm(Y ~ X)
  storage_b0[i] <- coef(mylm)[1]
  storage_b1[i] <- coef(mylm)[2]
  storage_rmse[i] <- summary(mylm)$sigma
}


layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart <- 0 #min(0,min(Y)) 
Ystop <- 500 #max(max(Y), 0)
Yrange <- Ystop - Ystart

plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col="gray",
     main="Regression Lines from many Samples\n Plus Residual Standard Deviation Lines")
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)
text(Xstart, Ystop-.25*Yrange, bquote(sqrt(MSE) == .(mean(storage_rmse))), pos=1)


for (i in 1:N){
  abline(storage_b0[i], storage_b1[i], col="darkgray")  
}
abline(beta_0, beta_1, col="green", lwd=3)
abline(beta_0+sigma, beta_1, col="green", lwd=2)
abline(beta_0-sigma, beta_1, col="green", lwd=2)
abline(beta_0+2*sigma, beta_1, col="green", lwd=1)
abline(beta_0-2*sigma, beta_1, col="green", lwd=1)
abline(beta_0+3*sigma, beta_1, col="green", lwd=.5)
abline(beta_0-3*sigma, beta_1, col="green", lwd=.5)

par(mai=c(1,.6,.5,.01))

  addnorm <- function(m,s, col="firebrick"){
    curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
    lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
    lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
    lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
    lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
    lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
    lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
    lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
    legend("topleft", legend=paste("Std. Error = ", round(s,3)), cex=0.7, bty="n")
  }

# Inference in linear regression, the gray bands are the collection of 2000 regression lines (we look at one of the samples of data, but the whole distribution of means)
# We have the most accuracy and least variability we should make our prediction in the middle, we have an advantage because the variability around Ybar is less than it was when we just guessed

  h0 <- hist(storage_b0, 
             col="skyblue3", 
             main="Sampling Distribution\n Y-intercept",
             xlab=expression(paste("Estimates of ", beta[0], " from each Sample")),
             freq=FALSE, yaxt='n', ylab="")
  m0 <- mean(storage_b0)
  s0 <- sd(storage_b0)
  addnorm(m0,s0, col="green")
  
  h1 <- hist(storage_b1, 
             col="skyblue3", 
             main="Sampling Distribution\n Slope",
             xlab=expression(paste("Estimates of ", beta[1], " from each Sample")),
             freq=FALSE, yaxt='n', ylab="")
  m1 <- mean(storage_b1)
  s1 <- sd(storage_b1)
  addnorm(m1,s1, col="green")






# Test Statistics, t Distributions, and P-values
curve(dt(x, 3), from=-4, to=4, lwd=2)
curve(dnorm(x), add=TRUE, col="gray")
abline(h=0, v=c(-1,1), col=c("gray","orange","orange"), lwd=c(1,2,2))

pt(-1, 3)*2 


cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)

pt(-2.601, 48)*2



round(pt(-1.285, 13)*2,4)

round(pt(-2.991, 48)*2,4)



cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
confint(cars.lm)
```

MSE is the average box (mean squared). It is an estimate of sigma^2. It is Y direction variability around the line.
When you take the sqrt of a square you get a distance. The average length of distance.
Sigma^2 of B1, how muchc the slope can vary, is equal of the distribution of Y  (It's like the ssto but in the x direction)
Does it make sence that the variability of the slope is the variation of the difference of x and variton of the same for 
sigma^2_bi = sigma^2/ssum of (xi -xhat)^2 true value
s^2_b1 = MSE/sums(of Xi-xhat)^2 the estimated value. The bottom one is the variability of the run

What would happen to those grey lines. If I were to reduce the distance of the dots, the grey lines would increase (because the distance on x is the denominator). We introduce more variability in the slopes . If we have larger sample, the denominator grows so the variability in y decreases. 


[t distribution applet](http://byuimath.com/apps/normprobwitht.html)

The quantile of the t distribution is the t-value that yields a certain probability of being less than that value. In other words, the t-value that corresponds to a given percentile.

Shading only the left tail is called a percentile.
```{r eval=FALSE}
qt(1-0.05/2, 48)

cars.lm <- lm(dist ~ speed, data=cars)
.97.5
```


Change the percentile (area) to be 0.975 (97.5 percentile)

If the middle is filled it's the confidence interval

```{r eval=FALSE}
# Set up the function:
tprob <- function(t=1, df=3, show.normal=TRUE, xlim=c(-4,4)){
  curve(dt(x, df), from=xlim[1], to=xlim[2], lwd=2)
  xlo = seq(xlim[1], -abs(t), length.out=100)
  xhi = seq(abs(t), xlim[2], length.out=100)
  polygon(c(xlo[1],xlo,xlo[100]), c(0,dt(xlo,df),0), col="#128b37", border=NA)
  polygon(c(xhi[1],xhi,xhi[100]), c(0,dt(xhi,df),0), col="#128b37", border=NA)  
  abline(h=0, v=c(-abs(t),abs(t)), col=c("gray","orange","orange"), lwd=c(1,3,3))
  text(xlo[1], dt(.5,df), paste("Area = ", round(pt(-abs(t), df)*2,4)), pos=4)
  if(show.normal){
    curve(dnorm(x), add=TRUE, col="gray")
  }
}

# Use the function
tprob(t=-2)
tprob(t=-8, xlim=c(-9,9))
tprob(t=-2, df=15)
```


<br>



###### Quizzes

The t-value from a regression in R is found by taking the "Estimate" and dividing by the "Std. Error". This is because R always uses the null hypothesis that the true parameter (for either the slope or the intercept) is zero. So the t-value is computed by t = (estimate - 0)/std. error = estimate/std. error.

Since a 95% confidence interval is obtained by the formula: estimate +/- margin of error, and the margin of error is given by (t*)(std. error) then we have:

qt(1-0.05/2, 397) #gives the critical value for t* = 1.965957

and std. error = 0.4379 #as shown in the summary output

So, 

8.5621 - 1.965957 * 0.4379 #lower bound

8.5621 + 1.965957 * 0.4379 #upper bound

If you used instead

8.5621 +/- 2 * 0.4379 

then you would have still come close to the correct answer, but it would not be as correct as using the actual critical value from the t distribution with 397 degrees of freedom.

The margin of error can be found from the confidence interval by using (-7.453 - -0.947)/2 = -3.253. Similarly, the estimate of the slope can be found by finding the middle of the confidence interval (-7.453 + -0.947)/2 = -4.2.

Then, the critical value of the margin of error can be found using qt(1-0.05/2, 100-2) = 1.984467.

This allows us to recover just the standard error of the slope, which is margin of error / critical value = -3.253/1.984467 = -1.639231.

Thus, t = (estimate - hypothesized)/std. error = (-4.2 - -10)/-1.639231 = -3.538244, and the corresponding p-value is clearly very small, but can be computed exactly by pt(-3.538244, 98)*2 which gives p-value = 0.0006175379.

```{r eval=FALSE}
# Mobius
library(Ecdat)
cl.lm <- lm(tsales~hourspw, data=Clothing)
summary(cl.lm)
ggplot(data=Clothing, mapping=aes(y=tsales, x=hourspw)) +
  geom_point(pch=16, color="skyblue") +
  geom_smooth(method="lm", se=F, formula=y~x)

# test statistic esimate b0-B0(hyp)/sd
(1745-1500)/67479
# pvalue (both sides)2*pt(-abs(t), df)
2*pt(-abs(0.003630759), 398)

(43884-35000)/3320 # slope
2*pt(-abs(2.675904), 398)

t.lm <- lm(sqrt(sqrt(tsales))~log(hourspw), Clothing)
summary(t.lm)
ggplot(data=Clothing, mapping=aes(y=sqrt(sqrt(tsales)), x=log(hourspw))) +
  geom_point(pch=16, color="skyblue") +
  geom_smooth(method="lm", se=F, formula=y~x)

boxCox(cl.lm)

# Assessment


```




<br>

####  5. Confidence and Prediction Intervals for Yhath

The t distribution is a little more spread out not as central. As df increases gets closer to normal.

predict a y value is very different than how confident we are about the average y value
prediction is almost always wider
rare we use confidence

##### Class Code

Tip if having to determine patterns look at the interaction plot

How do we find the spread.
The square root of MSE
Here's the summary output where's the SSE
By using teh degrees of freedom and resdiuals std error you can get back to 

curve(dt(x, 3), from=-4, to=4, lwd=2)
curve(dnorm(x), add=TRUE, col="gray")
abline(h=0, v=c(-1,1), col=c("gray","orange","orange"), lwd=c(1,2,2))

pt(-1, 3)*2 


cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)

pt(-2.601, 48)*2



round(pt(-1.285, 13)*2,4)

round(pt(-2.991, 48)*2,4)



cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
confint(cars.lm)

confident about where the line will be
but we predict where the data will land

The margin of error can be found from the confidence interval by using (-7.453 - -0.947)/2 = -3.253. Similarly, the estimate of the slope can be found by finding the middle of the confidence interval (-7.453 + -0.947)/2 = -4.2.
The critical value is what qt is finding. 
97.5 is tstar the critical value or what qt finds
t* is a theoretical distribution, t depends on the data
std error is theoretical, resudial standard error based on data
the std errors are based on the theory of sampling distribution (measure of how mucm all samples deviate from the truth)
Residual Standard error is based on your data.

Show to a peer how to compute "by hand" the p-values for both the intercept and slope terms in the summary(...) output.
2*pt(-abs(t), df) t is act - mean/std error

std error is theoretical, resudial standard error based on data Y
critical value theoretical, t value based on data Y
tscore? is bigger than tstar then you pvalue is smaller than level of significance
t = tscore = ttest Y
t* is critical value
Is tscore, ttest?
who gives alpha is tstar? alpha represents the significance
pt gives tvalue, qt gives tstar Y
ttest gives both tvalue pvalue for a null hypothesis of 0

denominator $s_b1$ its's like an ssto in the x direction <br>
10) std error (which?) measures for every other sample the variabilty

```{r eval=FALSE}
library(ggplot2)
cars.lm <- lm(dist~ speed, data=cars)
summary(cars.lm)
confint(cars.lm)

# SSE
15.38^2
*48 

#SSTO
# SSTO = SSE + SSR
R^2 = SSR/SSTO = 1- SSE= SSTO
# Have to be able to go forward and backward with regression output most in cheet sheet
```



figure out how to say these

```{r eval=FALSE}
#Faithful
plot(waiting ~ eruptions, data=faithful, pch=21, col="darkgray")
faith.lm <- lm(waiting ~ eruptions, data=faithful)
abline(faith.lm, col="darkgray", lwd=2)
abline(v=seq(1.2, 5, 0.5), lty=2, col="gray")
abline(h=seq(50,905, 10), lty=2, col="gray")

ggplot(faithful, aes(x=eruptions, y=waiting)) +
  geom_point(fill="skybule", color="darkgray", pch=21) +
  geom_smooth(method="lm", formula=y~x, se=T, color="darkgray")

View(faithful)
?faithful
# confidence is always narrow middle less on the ends it's for the line

# This code allows us to accurately (or at least with understanding of our inaccuracy) predict the time to the next eruption.
mypreds <- predict(faith.lm, data.frame(eruptions=1.967), interval="prediction")
# for prediction is for the dots
# prediction always wider it accounts for band and dots
# Notice that your prediction interval is about (not quite, but close) to 2 residual standard errors wider on each end than the confidence interval. Discuss with a peer why you think that is the case
#At xbar the lowest variability
myc <- predict(faith.lm, data.frame(eruptions=1.967), interval="confidence")
# difference between xi and xh (xh might be in the data set, it's a new x)
# MSE measures variance of the data around the line, the average


# creates a condfidence band, the true line falls in there
# but the data doesn't fall in there. We believe the average falls in 
# Who collected this, 1.967
# if I put conf on first it'll be hidden by prediction
ggplot(faithful, aes(x=eruptions, y=waiting)) +
  geom_point(fill="skyblue", color="darkgray", pch=21) +
  geom_smooth(method="lm", formula=y~x, se=T, color="darkgray") +
  geom_segment(aes(x=1.967, xend=1.967, y=mypreds[2], yend=mypreds[3], color = "Prediction", lwd=3)) + # se is standard error
  geom_segment(aes(x=1.967, xend=1.967, y=myc[2], yend=myc[3], color = "Confidence", lwd=3)) 

   
```




```{r}
# Notes
# pt takes a quantile  or t statistic
# z-score times margin of error sd
# pt(quantile, df)
# qt(percentil, df) for pvalue
# pt only gives us the left tail it has to be on the negative side -abs value or put in a negative number and then you don't have to use -abs
# left-tailed (pt(-abs(tvalue), degreeoffreedom)) double it to get both sides
# now you how to get the p-valueu compute it or use the applet from 221
pt(-abs((3.9324-5)/.4155), 48)*2
# if our estimate is closer to the truth it'l be a bigger pvalue because our estimate is closer to the new hypothesis than the zero hypothesis it's more likely to happen so # the p-value increases
pt(-abs(3.932-3.8))*2

```

An interesting hypothesis: is the intercept the actual msrp
automatic values only test the null hypothesis<br>
You can calculate both the basic calculation my estimate - /sd<br>
difference between the std error the tvalue is the <br>
t-value is how many std errors the estimate is from H0:B0=0<br>
What is the probability of being that far away (maybe p-value)<br>
Do one percent things happen--I got married.<br>
It could have happened but it’s not very likely.<br>
If this impossible thing is occurring my belief system is wrong.<br>
There must be another reason this happens.<br>
How could this happen if there was a god. God didn't let that happen, man chose this it's all on uspop God forbid it, man did it anyway.<br>
When something impossible is happening we actual decide we were wrong reject the null and accept an alternate hypothesis.<br>
```{r eval=FALSE}
round(pt(-1.285, 13)*2,4)
qt(1-0.05/2, 48)
```

 
If we ignore the two tails we .05 is alpha 1-alpha over 2 that's confidienc 95 if alphas point 1 then 90 for ronfint<br>
significance is on the outside and confidence is on the outside<br>
what people see verses what we have on the inside<br>
t-value is estimate-  over std error<br>
the test statistic (comes from data) and the pvalue have 1to1 rational()<br>
critical value is upt to a point from left<br>


Confidence is for estimating the mean. Prediction is for estimating an individual. Learning when to use which takes practice. Confidence is always for mean average.

Predict Interval says the actual temp or eruption will fall between _

```{r}
 qt(1-0.1/2,13)
```

t* is a test statistic it's testing how far from the value<br>
critical value<br>
once I know my confidence I can find my t-value<br>
t* a choice my confidence<br>
test statics t- comes from data and my 
critial value is how far<br>

cmy test statics has to get to reject the null, alpha<br>
test statistic to tstar<br>
our critical value is 1.77 at 90% roszihen ig less than 01 is level of significance<br>

t if tscore values in tales, reject null
if tscore in confidence, fail to reject the null
probability is bigger than the absolute value of t
tscore is bigger than tstar then you pvalue is smaller than level of signific
who gives alpha is tstar
who gives pvalue the test statistic or tvalue
```{r eval=FALSE}
cars.lm <- (dist ~ speed, cars)
summary(cars.lm)
confint(cars.lm)
```

estimate intercept is little b0 which equals Ybar - b1X   if I have some data, and I know where the middle of the data is with regards to x the ybar xbar is the middle of the data.<br>
What it's saying is slopes rise over run x is run, if you know how far you're running from zero, then how far should you fall. You should fall the slope xbar times after going xbar times over you'll be at 0 ybar minus the slope times xbar.<br>
its's like an ssto in the x direction <br>
std error measures for every other sample the variabilty<br>
<br>
confint gives 2.5 the lower bound and 97.5 upper boiund these two numbers are centered around the estimate 95 percent confident it's between here and here<br>
What matters is how spread out the x-values are and the y. That information, that dna, is always found in one sample. When the slope varies a lot, the variability of the y intercept changes. If we select data at the ends, there's more variability than at the middle.<br>
<br>
If you combine x and y, variances are additive, so Var[x+y] = var[x] + var[y]<br>
sigma^2_xbar = sigma^2/n<br>
The farther we get from the interecept the more variability we have. (The origin of the line of information we have assume it startss at the origin, 0 space and 0 time). Carbon dating, most people just throw out the date but not the confidence interval.<br>



```{r eval=FALSE}
# Wed 2-1
lm.u <- lm(gasbill ~ temp, data=Utilities)
summary(lm.u)

lm.u.sqsq <- lm(sqrt(sqrt(gasbill)) ~ temp, data=Utilities)
summary(lm.u.sqsq)
plot(lm.u.sqsq, which=1)

b <- coef(lm.u.sqsq)

pred.u <- predict(lm.u, data.frame(temp=30), interval="prediction")
pred.u.sqsq <- predict(lm.u.sqsq, data.frame(temp=30), interval="prediction")^4

pred.u2 <- predict(lm.u, data.frame(temp=70), interval="prediction")
pred.u.sqsq2 <- predict(lm.u.sqsq, data.frame(temp=70), interval="prediction")^4


plot(gasbill ~ temp, data=Utilities)
abline(lm.u, col="hotpink")
abline(225.7804 + 26.45, -2.9704, lty=2, col="hotpink")
abline(225.7804 - 26.45, -2.9704, lty=2, col="hotpink")
curve((b[1] + b[2]*x)^4, add=TRUE, col="skyblue", lwd=2)
curve((b[1]+0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
curve((b[1]-0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
abline(h=pred.u, lty=2, col="hotpink")
abline(h=pred.u.sqsq, lty=2, col="skyblue")
abline(v=c(30,70), lty=2)
abline(h=pred.u2, lty=2, col="hotpink")
abline(h=pred.u.sqsq2, lty=2, col="skyblue")

plot(lm.u, which=1)
View(Utilities)

library(car)
boxCox(lm.u)



ggplot(Utilities, aes(x=temp, y=gasbill)) + 
  geom_point() + 
  geom_smooth(method="lm", formula=y~x, se=T) +
  stat_function(fun=function(x) (b[1] + b[2]*x)^4) + 
  geom_segment(aes(x=30,xend=30, y=pred.u[2], yend=pred.u[3]), alpha=0.01, lwd=3, col="hotpink") + 
  geom_segment(aes(x=30,xend=30, y=pred.u.sqsq[2], yend=pred.u.sqsq[3]), alpha=0.01, lwd=2, col="skyblue") + 
  geom_segment(aes(x=70,xend=70, y=pred.u2[2], yend=pred.u2[3]), alpha=0.01, lwd=3, col="hotpink") + 
  geom_segment(aes(x=70,xend=70, y=pred.u.sqsq2[2], yend=pred.u.sqsq2[3]), alpha=0.01, lwd=2, col="skyblue") 
  


library(mosaic)
library(tidyverse)
lm.u <- lm(gasbill ~ temp, data=Utilities)
summary(lm.u)
lm.u.sqsq <- lm(sqrt(sqrt(gasbill)) ~ temp, data=Utilities)
summary(lm.u.sqsq)
plot(lm.u.sqsq, which=1)

# bring curve back
b <- coef(lm.u.sqsq)

pred.u <- predict(lm.u, data.frame(temp=30), interval="prediction")
pred.u
pred.u.sqsq <- predict(lm.u.sqsq, data.frame(temp=30), interval="prediction")^4

pred.u <- predict(lm.u, data.frame(temp=30), interval="prediction")
pred.u.sqsq <- predict(lm.u.sqsq, data.frame(temp=30), interval="prediction")^4


# residualstandard error same units
# first plot which 2 is mirrored
plot(gasbill ~ temp, data=Utilities)
abline(lm.u, col="hotpink")
#abline(225.7804 + 26.45, -2.9704, lty=2, col="hotpink")
#abline(225.7804 - 26.45, -2.9704, lty=2, col="hotpink")
curve(((b[1] + b[2]*x)^4), add=TRUE, col="skyblue")
#curve(((b[1] + 0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2))
#curve(((b[1] - 0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2))
abline(h=pred.u, lty=2, col="hotpink")
abline(h=pred.u.sqsq, lty=2, col="skyblue")
abline(v=30, lty=2, col="skyblue")


# because I made it transparent
ggplot(Utilities, aes(x=temp, y=gasbill)) +
  geom_point() +
  geom_smooth(method="lm", formula=y~x, se=F) +
  stat_function(fun=function(x) (b[1]+b[2]*x)^4) +
  


# we'd go look up the ones that look like they might be a problem and remove the ones we can


# when you don't have linearity and lack constant ? You have nothing
plot(lm.u, which=1)
install.packages("cars")
library(cars)
boxCox(lm.u)



```

ylim statement to extend the graph to show the prediction intervals
change conclusion to have a prediction interval
bring prediction interval for bought car and the interval for sell the car
discuss them in conclusion

##### Quizzes

 $ \underbrace{E\{Y\}}_{\substack{\text{true mean} \\ \text{y-value}}} = \underbrace{\overbrace{\beta_0}^\text{y-intercept} + \overbrace{\beta_1}^\text{slope} X}_\ $
 
 $\underbrace{Y_i}_\text{Scale}  = \beta_0 + \beta_1 \underbrace{X_i}_{Length} + \epsilon_i \quad \text{where} \ \epsilon_i\sim N(0,\sigma^2)$

```{r eval=FALSE}
# Transformations
library(car)
?Davis

lm.davis <- lm(height ~ weight, data=Davis)
summary(Davis)

ggplot(Davis, aes(y=height, x=weight)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Exercising Individuals", xlab="weight", ylab="height")

plot(lm.davis, which=1)

install.packages("alr4")
library(alr4)
View(BGSall)

BG.lm <- lm(HT18 ~ HT2, data=BGSall)
summary(BG.lm)

ggplot(BGSall, aes(y=HT18, x=HT2)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Height from Age 2 to Age 18", xlab="HT2", ylab="HT18")

confint(BG.lm)
plot(BG.lm, which=1:2)

predict(BG.lm, data.frame(HT2=33*2.54), interval="prediction")
(1.4441-2)/0.1901
2*pt(-abs(-2.92425), df=134)




library(alr3)
wblake <- load(file = "C:/Users/rizen/Downloads/wblake.rda")
View(wblake)


wb.lm <- lm(Scale ~ Length, data=wblake)
summary(wb.lm)
ggplot(wblake, aes(y=Scale, x=Length)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Radius of Key Scale increase by Length of Fish", xlab="Length", ylab="Scale")

sqrt.lm <-lm(sqrt(Scale) ~ Length, data=wblake)
summary(sqrt.lm)
b <- coef(sqrt.lm)
ggplot(wblake, aes(y=sqrt(Scale), x=Length)) + 
  geom_point() +
  geom_smooth(method="lm", forumla = y~x, se=F) +
  labs(title="Radius of Key Scale increase by Length of Fish", xlab="Length", ylab="Scale")


plot(gasbill ~ temp, data=Utilities)
abline(lm.u, col="hotpink")
abline(225.7804 + 26.45, -2.9704, lty=2, col="hotpink")
abline(225.7804 - 26.45, -2.9704, lty=2, col="hotpink")
curve((b[1] + b[2]*x)^4, add=TRUE, col="skyblue", lwd=2)
curve((b[1]+0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
curve((b[1]-0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
abline(h=pred.u, lty=2, col="hotpink")
abline(h=pred.u.sqsq, lty=2, col="skyblue")
abline(v=c(30,70), lty=2)
abline(h=pred.u2, lty=2, col="hotpink")
abline(h=pred.u.sqsq2, lty=2, col="skyblue")


plot(Scale ~ Length, data=wblake)
abline(wb.lm, col="hotpink")
curve(((ob[1]) + ob[2]*x)^2, add=TRUE, col="skyblue", lwd=2)

abline(h=6.19091, lty=2, col="hotpink")
abline(h=2.460408, lty=2, col="skyblue")
abline(v=250, lty=2, col="hotpink")
abline(h=9.848869, lty=2, col="hotpink")
abline(h=3.171672, lty=2, col="skyblue")


# More Back Transformations
lm.log <- lm(log(circumference) ~ age, data=Orange)
b.log <- coef(lm.log)
b.log[1]
b.log[2]

lm.sqrt <- lm(sqrt(circumference) ~ age, data=Orange)
b.sqrt <- coef(lm.sqrt)

lm.1oY <- lm(1/circumference ~ age, data=Orange)
b.1oY <- coef(lm.1oY)

lm.y <- lm(circumference ~ age, data=Orange)
b.y <- coef(lm.y)

lm.y2 <- lm(circumference^2 ~ age, data=Orange)
b.y2 <- coef(lm.y2)

lm.1oY2 <- lm(1/circumference^2 ~ age, data=Orange)
b.1oY2 <- coef(lm.1oY2)


# Base Graphic


plot(circumference ~ age, data=Orange, pch=16, col="orangered", main="Growth of Orange Trees", xlab="Age of Tree in Days", ylab="Circumference of Tree (mm)")

curve( exp(b.log[1] + b.log[2]*x), add=TRUE, col="red")
curve( (b.sqrt[1] + b.sqrt[2]*x)^2, add=TRUE, col="blue")
curve( 1/(b.1oY[1] + b.1oY[2]*x), add=TRUE, col="green")
curve( b.y[1] + b.y[2]*x, add=TRUE, col="gray")
curve( sqrt(b.y2[1] + b.y2[2]*x), add=TRUE, col="orange")
curve( 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), add=TRUE, col="forestgreen")

# ggplot Graphic

library(tidyverse)

ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  stat_function(fun=function(x) exp(b.log[1] + b.log[2]*x), aes(color="log(Y)")) + 
  stat_function(fun=function(x) (b.sqrt[1] + b.sqrt[2]*x)^2, aes(color="sqrt(Y)")) + 
  stat_function(fun=function(x) 1/(b.1oY[1] + b.1oY[2]*x), aes(color="1/Y")) + 
  stat_function(fun=function(x) b.y[1] + b.y[2]*x, aes(color="Y")) + 
  stat_function(fun=function(x) sqrt(b.y2[1] + b.y2[2]*x), aes(color="Y^2")) + 
  stat_function(fun=function(x) 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), aes(color="1/Y^2")) + 
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( ) 

library(car)
boxCox(lm.y)


YoungOrange <- filter(Orange, age < 1200)


lm.log <- lm(log(circumference) ~ age, data=YoungOrange)
b.log <- coef(lm.log)

lm.sqrt <- lm(sqrt(circumference) ~ age, data=YoungOrange)
b.sqrt <- coef(lm.sqrt)

lm.1oY <- lm(1/circumference ~ age, data=YoungOrange)
b.1oY <- coef(lm.1oY)

lm.y <- lm(circumference ~ age, data=YoungOrange)
b.y <- coef(lm.y)

lm.y2 <- lm(circumference^2 ~ age, data=YoungOrange)
b.y2 <- coef(lm.y2)

lm.1oY2 <- lm(1/circumference^2 ~ age, data=YoungOrange)
b.1oY2 <- coef(lm.1oY2)



ggplot(Orange, aes(x=age, y=circumference)) + 
  geom_point(color="orangered") +
  geom_vline(aes(xintercept=1200)) + 
  stat_function(fun=function(x) exp(b.log[1] + b.log[2]*x), aes(color="log(Y)")) + 
  stat_function(fun=function(x) (b.sqrt[1] + b.sqrt[2]*x)^2, aes(color="sqrt(Y)")) + 
#  stat_function(fun=function(x) 1/(b.1oY[1] + b.1oY[2]*x), aes(color="1/Y")) + 
  stat_function(fun=function(x) b.y[1] + b.y[2]*x, aes(color="Y")) + 
  stat_function(fun=function(x) sqrt(b.y2[1] + b.y2[2]*x), aes(color="Y^2")) + 
#  stat_function(fun=function(x) 1/sqrt(b.1oY2[1] + b.1oY2[2]*x), aes(color="1/Y^2")) + 
  ylim(c(0,300)) + 
  labs(title="Growth of Orange Trees", x="Age of Tree in Days", y="Circumference of Tree (mm)") + 
  theme_bw( ) 




ob <- coef(wb.lm)
summary(wb.lm)

curve((b[1]+0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
curve((b[1]-0.2258 + b[2]*x)^4, add=TRUE, col="skyblue", lty=2, lwd=2)
abline(h=pred.u, lty=2, col="hotpink")
abline(h=pred.u.sqsq, lty=2, col="skyblue")
abline(v=c(30,70), lty=2)
abline(h=pred.u2, lty=2, col="hotpink")
abline(h=pred.u.sqsq2, lty=2, col="skyblue")

boxCox(wb.lm)
confint(wb.lm)
plot(wb.lm, which=1:2)
plot(sqrt.lm, which=1:2)

confint(wb.lm)
confint(sqrt.lm)

wb.preds <- predict(wb.lm, data.frame(Length=250), interval="prediction")
sqrt.preds <- predict(sqrt.lm, data.frame(Length=250), interval="prediction")

install.packages("Ecdat")
library(Ecdat)
library(car)
View(Caschool)
mylm <- lm(testscr ~ mealpct, data=Caschool)
plot(testscr ~ mealpct, data=Caschool)
abline(mylm)
summary(mylm)
confint(mylm, level=.95)
plot(mylm, which=1:2)
plot(mylm$residuals, ylab="Residuals")

View(Clothing)
mylm1 <- lm(tsales ~ hourspw, data=Clothing)
mylm2 <- lm(tsales ~ hourspw, data=Clothing2)
plot(tsales ~ hourspw, data=Clothing2)
abline(mylm)
summary(mylm)
confint(mylm, level=.95)
plot(mylm, which=1:2)
plot(mylm$residuals, ylab="Residuals")
pt(-abs(1500), 398)
Clothing2 <- Clothing[-397,]
bc1 <-boxCox(mylm)
bc$
library(MASS)
bc <- boxcox(mylm1)
bc$
#pull the max lambda
lamdba <-bc$x[which.max(bc$y)]
lamdba
boxcox()
BoxCox(mylm, lam)
library(car)
boxCox(mylm)


library(forecast)
BoxCox(mylm)

View(Loblolly)
ha.lm <- lm(height~age, data=Loblolly)
summary(ha.lm)
plot(height~age, data=Loblolly)
abline(ha.lm)
plot(ha.lm, which=1)
```




#### 6. Different Types of Regression  Models

All models are wrong. Some are useful. ~Box<br>
If you can't visualize it. Don't trust it. (in Stats)

[Desmos](https://www.desmos.com/calculator)<br>
New Models<br>


* Quadratic * <br>
The numberline is made of uncountable infinite dots.
In math the highest order wins. The shape is determined by it's highest ordered term. So as we move away from zero on x, x^2 has more influence.<br>
If you can place the vertex (b1), and y intercept (b0), you can place it.<br>
$ax^2 + bX + c$ $a$ = quadratic, $b$ = slope term, $c$ = constant term<br>
You cannot just square X if you actually have a parabola in the data<br>
In this model you have to have a color label.
$$Y_i = \overbrace{\underbrace{\beta_0 + \beta_1 X_i + \beta_2 X_i^2}_{E\{Y_i\}}}^\text{Quadratic Model} + \epsilon_i$$

* $\beta_0$ Y-intercept of the Model<br>
* $\beta_1$ Controls the x-position of the vertex of the parabola by $\frac{-\beta_1}{2\cdot\beta_2}$
* $\beta_2$ Controls the concavity and “steepness” of the Model: negative values face down, positive values face up; large values imply “steeper” parabolas and low values imply “flatter” parabolas. Also involved in the position of the vertex, see $\beta_1$'s explanation.<br>

##### Class Code

###### Sim Linear Regression

```{r eval=FALSE}
  ## Simulating Data from a Regression Model
  ## This R-chunk is meant to be played in your R Console.
  ## It allows you to explore how the various elements
  ## of the regression model combine together to "create"
  ## data and then use the data to "re-create" the line.

  set.seed(101) #Allows us to always get the same "random" sample
                #Change to a new number to get a new sample

  n <- 30 #set the sample size

  X_i <- runif(n, 15, 45) 
    #Gives n random values from a uniform distribution between 15 to 45.

  beta0 <- 3 #Our choice for the y-intercept. 

  beta1 <- 1.8 #Our choice for the slope. 

  sigma <- 2.5 #Our choice for the std. deviation of the error terms.


  epsilon_i <- rnorm(n, 0, sigma) 
    #Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.

  Y_i <- beta0 + beta1*X_i + epsilon_i 
    #Create Y using the normal error regression model

  fabData <- data.frame(y=Y_i, x=X_i) 
    #Store the data as data

  View(fabData) 
  

  #In the real world, we begin with data (like fabData) and try to recover the model that 
  # (we assume) was used to created it.

  fab.lm <- lm(y ~ x, data=fabData) #Fit an estimated regression model to the fabData.

  summary(fab.lm) #Summarize your model. 

  plot(y ~ x, data=fabData) #Plot the data.

  abline(fab.lm) #Add the estimated regression line to your plot.


  # Now for something you can't do in real life... but since we created the data...

  abline(beta0, beta1, lty=2) 
    #Add the true regression line to your plot using a dashed line (lty=2). 

  legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n") 
    #Add a legend to your plot specifying which line is which.


```
###### Sim Quadratic Regression
```{r eval=FALSE}
library(ggplot2)
## Simulating Data from a Regression Model
## This R-chunk is meant to be played in your R Console.
## It allows you to explore how the various elements
## of the regression model combine together to "create"
## data and then use the data to "re-create" the line.

#Allows us to always get the same "random" sample
#Change to a new number to get a new sample
set.seed(101) 

#set the sample size
n <- 120 

# random uniform gives n random values in a uniform distribution from 15 to 45
#Gives n random values from a uniform distribution between 0 to 10.
X_i <- runif(n, 0, 10) 

# Set   equation
beta0 <- -.45 #Our choice for the y-intercept. 
beta1 <- 1.28 #Our choice for the slope. 
beta2 <- -0.1
sigma <- .25 #Our choice for the std. deviation of the error terms.
#Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
epsilon_i <- rnorm(n, 0, sigma) 
    
#Create Y using the normal error quadratic regression model  
Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + epsilon_i 
    
#Store the data as data
fabData <- data.frame(y=Y_i, x=X_i) 
#View(fabData) 
  

#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.
fab.lm <- lm(y ~ x + I(x^2), data=fabData) #Fit an estimated regression model to the fabData.
b <- coef(fab.lm)
summary(fab.lm) #Summarize your model. 
  
ggplot(fabData, aes(y=Y_i, x=X_i)) + 
  ylim(0,4) +
  geom_point(pch=21, bg="gray83", color="skyblue") +
  #geom_smooth(method="lm", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, lty=1, aes(color="Yhat")) +
  stat_function(fun = function(x) beta0 + beta1*x + beta2*x^2, lty=2, aes(color="E{Y}")) +
  labs(title="Quadratic Regression Relation Diagram") +
  scale_colour_manual("Legend", values = c("Skyblue", "Pink")) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  theme_minimal()
  
```


###### Sim Cubic Regression
```{r eval=FALSE}
library(ggplot2)
## Simulating Data from a Regression Model
## This R-chunk is meant to be played in your R Console.
## It allows you to explore how the various elements
## of the regression model combine together to "create"
## data and then use the data to "re-create" the line.

#Allows us to always get the same "random" sample
#Change to a new number to get a new sample
set.seed(121) 

#set the sample size
n <- 30 

# random uniform gives n random values in a uniform distribution from 15 to 45
#Gives n random values from a uniform distribution between 0 to 10.
X_i <- runif(n, -2, 4) 

# Set   equation
beta0 <- 2.2 #Our choice for the y-intercept. 
beta1 <- -4 #Our choice for the slope. 
beta2 <- -2.1
beta3 <- 1
sigma <- 1.8 #Our choice for the std. deviation of the error terms.
#Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
epsilon_i <- rnorm(n, 0, sigma) 
    
#Create Y using the normal error quadratic regression model  
Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + beta3*X_i^3+ epsilon_i 
    
#Store the data as data
fabData <- data.frame(y=Y_i, x=X_i) 
#View(fabData) 
  

#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.
fab.lm <- lm(y ~ x + I(x^2) + I(x^3), data=fabData) #Fit an estimated regression model to the fabData.
b <- coef(fab.lm)
summary(fab.lm) #Summarize your model. 


##### HOW GET DOTTED LINES IN LEGEND #####
ggplot(fabData, aes(y=Y_i, x=X_i)) + 
  ylim(-8,8) +
  geom_point(pch=21, bg="gray83", color="skyblue") +
  #geom_smooth(method="lm", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2 + b[4]*x^3, lty=1, aes(color="Yhat")) +
  stat_function(fun = function(x) beta0 + beta1*x + beta2*x^2 + beta3*x^3, lty=2, aes(color="E{Y}")) +
  labs(title="Cubic Regression Relation Diagram") +
  scale_colour_manual("Legend", values = c("Skyblue", "Pink")) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  theme_minimal()
  
```
###### Lim Two-Line Regression

```{r eval=FALSE}
library(ggplot2)
## Simulating Data from a Regression Model
## This R-chunk is meant to be played in your R Console.
## It allows you to explore how the various elements
## of the regression model combine together to "create"
## data and then use the data to "re-create" the line.

#Allows us to always get the same "random" sample
#Change to a new number to get a new sample
set.seed(121) 

#set the sample size
n <- 30 

# random uniform gives n random values in a uniform distribution from 15 to 45
#Gives n random values from a uniform distribution between 0 to 10.
X_1 <- runif(n, 29, 50) 
X_2i <- sample(c(0,1), n, replace=TRUE) # what does replace = TRUE do
     #Gives n 0's and 1's randomly

# Set   equation
beta0 <- -12 #Our choice for the y-intercept. 
beta1 <- 1.35 #Our choice for the slope. 
beta2 <- 70
beta3 <- -1.65
sigma <- 1.8 #Our choice for the std. deviation of the error terms.
#Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
epsilon_i <- rnorm(n, 0, sigma) 
    
#Create Y using the two-lines regression model

Y_i <- beta0 + beta1*X_1 + beta2*X_2i + beta3*X_1*X_2i + epsilon_i 
    
#Store the data as data
fabData <- data.frame(y=Y_i, x1=X_1, x2=X_2i) 

  

#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.
fab2.lm <- lm(y ~ x1 + x2 + x1:x2, data=fabData) #Fit an estimated regression model to the fabData.
b <- coef(fab2.lm)
summary(fab2.lm) #Summarize your model. 
#View(fabData)


ggplot(fabData, aes(x = X_1, y =Y_i, color=as.factor(X_2i))) + 
  ylim(29,60) +
  geom_point(pch=21, bg="gray83") +
  #geom_smooth(method="lm", se=F, formula = y ~ poly(x, 2)) + #easy way, but the more involved manual way using stat_function (see below) is more dynamic.
  stat_function(fun = function(x) b[1] + b[2]*x, lty=1, aes(color="Yhat 1")) +
  stat_function(fun = function(x) (b[1] + b[3]) + (b[2] + b[4])*x, lty=1, aes(color="Yhat 2")) +  
  stat_function(fun = function(x) beta0 + beta1*x, lty=2, aes(color="E{Y} 1")) +
  stat_function(fun = function(x) (beta0 + beta2) + (beta1 + beta3)*x, lty=2, aes(color="E{Y} 2")) +
  labs(title="Two Quadratics Regression Relation Diagram") +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  theme_minimal()


#In the real world, we begin with data (like fabData) and try to recover the model that (we assume) was used to created it.
quad2.lm <- lm(qsec ~ disp + I(disp^2) + am + disp:am + I(disp^2):am,
fab.lm <- lm(y ~ x + I(x^2), data=fabData) #Fit an estimated regression model to the fabData.
b <- coef(fab.lm)
summary(fab.lm) #Summarize your model.

```

##### Quizzes

```{r, eval=FALSE}
#Mobius
library(mosaic)
View(Utilities)
ut.lm <- lm(gasbill ~ month + I(month^2), data=Utilities)
b <- coef(ut.lm)
summary(ut.lm)
ggplot(Utilities, aes(y=gasbill, x=month))+
  geom_point() +
  geom_hline(yintercept=sept.mn) +
  stat_function(fun = function(x) b[1] + b[2]*x +b[3]*x^2, lty=1, aes(color="blue")) +
  labs(title="Single Residence in Minnesota", xlab="Month of the Year", ylab="Monthly Gas Bill (US Dollars") +
  theme_minimal()

plot(ut.lm, which = 1:2)
plot(ut.lm$residuals)

predict(ut.lm, data.frame(month=9), interval="prediction")
sept.ut <- Utilities %>% filter(month == 9)
sept.mn <- mean(sept.ut$gasbill)
View(sept.ut)

38.84658 - sept.mn
mt2.lm <- lm(mpg ~ qsec + am + qsec:am, data=mtcars)
b <- coef(mt2.lm)
summary(mt2.lm)

ggplot(mtcars, aes(y=mpg, x=qsec, color=factor(am))) +
  geom_point() +
  stat_function(fun=function(x) b[1]+b[2]*x, color="skyblue") +
  stat_function(fun=function(x) (b[1]+b[3]) + (b[2]+b[4])*x, color="orange") +
  labs(title="Transmission type influences gas mileage, mtcars", xlab="Quarter mile time of Vehicle", ylab="Gas Mileage of Vehicle") +
  scale_color_manual(name="Transmission (am)", values=c("skyblue","orange")) +
  theme_minimal()
                  
plot(mt2.lm, which = 1:2)
plot(mt2.lm$residuals)


library(Ecdat)
View(Wages1)
qu.lm <- lm(wage ~ school + I(school^2), data=Wages1)
b <- coef(qu.lm)
summary(qu.lm)

ggplot(Wages1, aes(y=wage, x= school)) +
  geom_point() +
  stat_function(fun=function(x) b[1] + b[2]*x + b[3]*x^2)
View(Loblolly)
qu.lm <- lm(height ~ age + I(age^2), data=Loblolly)

cub.lm <- lm(height ~ age + I(age^2) + I(age^3), data=Loblolly)
plot(qu.lm, which=1)
plot(cub.lm, which=1)

# mtcas 2 parabolics,
View(mtcars)
library(ggplot2)
        
quad2.lm <- lm(qsec ~ disp + I(disp^2) + am + disp:am + I(disp^2):am, data=mtcars)
summary(quad2.lm)
b <- coef(quad2.lm)


ggplot(mtcars, aes(x=disp, y=qsec, col=as.factor(am))) +
  geom_point() +
  stat_function(fun=function(x) b[1] + b[2]*x + b[3]*x^2, aes(color="0")) +
  stat_function(fun=function(x) (b[1] + b[4]) + (b[2] + b[5])*x + 
                  (b[3] + b[6])*x^2, aes(color="1")) +
  labs(title="The interaction of displacement andh transmisison type on quarter mile time", xlab="Displacement", ylab="Qsec") +
  scale_color_manual(name="Transmission (am)", values=c("skyblue","orange")) +
  theme_minimal()

plot(quad2.lm, which=1:2)
plot(quad2.lm$residuals)

# run both without a missing term

quad2.lm <- lm(qsec ~ disp + I(disp^2) + am + I(disp^2):am, data=mtcars)
summary(quad2.lm)

```

##### Quiz two Quadratics

```{r eval=FALSE}
# Simulate two quadratics
# y_i = b_0 + b_1X_i + b_2X_i^2 + b_3X_2 + b_4X_1X_2 + b_5X_i^2
library(ggplot2)
## Simulating Data from a Regression Model
## This R-chunk is meant to be played in your R Console.
## It allows you to explore how the various elements
## of the regression model combine together to "create"
## data and then use the data to "re-create" the line.

#Allows us to always get the same "random" sample
#Change to a new number to get a new sample
set.seed(111) 

#set the sample size
n <- 100 

# random uniform gives n random values in a uniform distribution from 15 to 45
#Gives n random values from a uniform distribution between 0 to 10.
X_i <- runif(n, -2, 2) 
X_2i <- sample(c(0,1), n, replace=TRUE) 

# Set   equation
beta0 <- -2 #Our choice for the y-intercept. 
beta1 <-  3 #Our choice for the slope. 
beta2 <-  4
beta3 <-  4
beta4 <-  2
beta5 <- -7
sigma <- .5 #Our choice for the std. deviation of the error terms.
#Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
epsilon_i <- rnorm(n, 0, sigma) 
    
#Create Y using the normal error quadratic regression model (don't forget beta3's X_2i)
Y_i <- beta0 + beta1*X_i + beta2*X_i^2 + beta3*X_2i + beta4*X_i*X_2i + beta5*(X_i^2)*X_2i + epsilon_i 
    
#Store the data as data
fabData <- data.frame(y=Y_i, x1=X_i, x2=X_2i) 
#View(fabData) 

quad2.lm <- lm(y ~ x1 + I(x1^2) + x2 + x1:x2 + I(x1^2):x2, data=fabData)
b <- coef(quad2.lm)
summary(quad2.lm)

  
ggplot(fabData, aes(y=y, x=x1, color=as.factor(x2))) + 
  geom_point(pch=16) +
  stat_function(fun = function(x) b[1] + b[2]*x + b[3]*x^2, lty=1, aes(color="Yhat1")) +
  stat_function(fun = function(x) (b[1] + b[4]) + (b[2] + b[5])*x + (b[3] + b[6])*(x^2), lty=1, aes(color="Yha2t")) +
  stat_function(fun = function(x) beta0 + beta1*x + beta2*x^2, lty=2, aes(color="E{Y}1")) +
  stat_function(fun = function(x) (beta0 + beta3) + (beta1 + beta4)*x + (beta2 + beta5)*(x^2), lty=2, aes(color="E{Y}2")) +
  labs(title="Quadratic Regression Relation Diagram") +
  scale_colour_manual("Legend", values = c("pink", "skyblue", "pink", "skyblue", "skyblue","pink")) +
  theme(legend.position = "bottom", legend.title = element_blank()) +
  theme_minimal()

# Could pass in the x2 and set it to 0 the first time, 1 the second
# stat_function(fun = function(x, x2=0) b[1] + b[2]*x + b[3]*x^2 + b[4]*x2 + b[5]*x*x2 + b[6]*x^2*x2, lty=1, aes(color="Yhat1")) +
  





```


#### 7. Lowess Curves

The residuals v fitted plot is essentially a lowess curve.

```{r, eval=FALSE}
#library(tidyverse)
#View(Utilities)

quad.lm <- lm(gasbill ~ month + I(month^2), data=Utilities)
summary(quad.lm)
plot(quad.lm, which=1)

plot(gasbill ~ month, data=Utilities)
lines(lowess(Utilities$month, Utilities$gasbill), col="red")
b <- coef(quad.lm)
curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE, col="blue")

ggplot(Utilities, aes(x=month, y=gasbill)) +
  geom_point() +
  geom_smooth() + #lowess is the default
  stat_function(b[1] + b[2]*x + b[3]*x^2)


X <- c(1, 4,  7,   8,  10, 20)

Y <- c(3, 5, 18, 13, 12,   1)

 

w2 <- c(.2,.4, 1, 1, 1, 1)

mylm <- lm(Y ~ X, weights=w2)

plot(Y ~ X, pch=21, bg=rgb(1-w2,1-w2,1-w2), col="orange")

abline(mylm)

```


##### Quizzes


```{r, eval=FALSE}
# Fit this model to the KidsFeet dataset. Use your model to answer the questions and re-create the graph shown below.
library(mosaic)
View(KidsFeet)

# quadratic
kfquad.lm <- lm(length ~ width + I(width^2), data=KidsFeet)
summary(kfquad.lm)
b <- coef(kfquad.lm)

ggplot(KidsFeet, aes(y=length, x=width)) +
  geom_point(pch=16) +
  stat_function(fun=function(x) b[1] + b[2]*x + b[3]*x^2, lty=1) +
  labs(title="KidsFeet dataset") +
  theme_minimal()

# 2 lines, width and width and sex interaction
kf2.lm <- lm(length ~ width + sex, data=KidsFeet)
summary(kf2.lm)
b <- coef(kf2.lm)

ggplot(KidsFeet, aes(y=length, x=width, color=sex)) +
  geom_point(pch=16) +
  stat_function(fun=function(x) b[1] + b[2]*x, lty=1, aes(color="B")) +
  stat_function(fun=function(x) (b[1] + b[3]) + b[2]*x, lty=1, aes(color="G")) +
  scale_colour_manual("sex", values = c("skyblue", "red", "skyblue", "red")) +
  labs(title="KidsFeet dataset") +
  theme(legend.justification = c(0,1), legend.position = c(0,1)) +
  theme_minimal()

# 2 quadratics
kf2q.lm <- lm(length ~ width + I(width^2) + sex + I(width^2):sex, data=KidsFeet)
summary(kf2q.lm)
b <- coef(kf2q.lm)

ggplot(KidsFeet, aes(y=length, x=width, color=sex)) +
  geom_point(pch=16) +
  stat_function(fun=function(x) b[1] + b[2]*x + b[3]*x^2, lty=1, aes(color="B")) +
  stat_function(fun=function(x) (b[1] + b[4]) + b[2]*x + (b[3] + b[5])*(x^2), lty=1, aes(color="G")) +
  scale_colour_manual("sex", values = c("skyblue", "red", "skyblue", "red")) +
  labs(title="KidsFeet dataset") +
  theme(legend.justification = c(0,1), legend.position = c(0,1)) +
  theme_minimal()

# simple 
kf.lm <- lm(length ~ width, data=KidsFeet)
summary(kf.lm)
b <- coef(kf.lm)

ggplot(KidsFeet, aes(y=length, x=width)) +
  geom_point(pch=16) +
  stat_function(fun=function(x) b[1] + b[2]*x, lty=1) +
  labs(title="KidsFeet dataset") +
  theme_minimal()

#2
View(Loblolly)

# simple
lbly <- lm(height ~ age, data=Loblolly)
summary(lbly)
b <- coef(lbly)

ggplot(Loblolly, aes(x=age, y=height)) +
  geom_point() +
  geom_smooth() + #lowess is the default
  stat_function(fun=function(x) b[1] + b[2]*x)


lbly2 <- lm(height ~ age + I(age^2), data=Loblolly)
summary(lbly2)
b <- coef(lbly2)

ggplot(Loblolly, aes(x=age, y=height)) +
  geom_point() +
  geom_smooth() + #lowess is the default
  stat_function(fun=function(x) b[1] + b[2]*x + b[3]*x^2)


#3
?Utilities
pairs(cbind(Res=lm3$res,Fit=lm3$fit,rbdata_adv), panel=panel.smooth, col=interaction(rbdata_adv$x4, rbdata_adv$x5, rbdata_adv$x8))


```

### fr

Any two samples should be close to the truth.
The secret of everything in validation is 
We run the lm on the first sample of data (training) . Lock the model in place.
We then run a new sample of data and see if the new sample of data seems to live where our model says it should have lived. We use the new model . With weather then difference is called a residual. If you kept using the same model. It puts different values of x into the model. You can simple  take the residual a bunch of times and get an r2 value. To see if it's consistenly good.

Uses two data sets. 


Copy code chunk, but stop before lm after we load the data. Then  go to the results section. We make a second set of data rbdata2. Grab a new set seed. We'v named it differently at rbdata2. 

Model validation session. 

You have to also draw, the true model. brother Saunders guess and two peers. You have to visualize there guess. It's in the file. Hopefully saw his guess and showed how he came up with the guess. 

Do not put second data set into an lm. NEVER


Here's what we do with second. get new see
created second data set from the data to rbdata2. (don't need to plot the second sample of data.)

Add a table, origianll guess and how it holds up.


The second sample of data set goes into prediction with the previous lm

easist if you use their names.

What do yht measure is our guess. We need to measure our guess agoinst the truth. The y sample in the second set of data.

How much the truth missed. Change into r squared with 1- the SSTO
If we measure the SSE, you could have missedso bad that you actually increased the data.Adjusted r squared. 

Hard part today is drawing the two competitiors guesses.


Turn it in and then. This peer review next week. you'll be assigned two peer reviews, you'll get to see the file for t he two people you guessed on. Just make sure they correctly represented in that file.


<br>

#### 10. Houses

  Remember numbers are the change from the intercept and do not include the significance of the change between each other.
  
  If we switch the baseline we can see the difference between the two other groups. Significance between the baseline.
  
  When looking at interactions look for number of obs deleted. Check to see whether the NAs deleted actually mean something like no alley.
         
         
```{r, eval=FALSE}
library(tidyverse)
train <- read.csv("Analyses//Advanced Linear Regression//house-prices-advanced-regression-techniques-1//train.csv", header=TRUE, stringsAsFactors=TRUE)

train2 <- train %>%
  mutate(Alley = as.character(Alley),
         Alley = ifelse(is.na(Alley), "No Alley", Alley),
         Alley = factor(Alley, levels=c("No Alley", "Grvl", "Pave"), ordered=TRUE)) %>%
  mutate(Fence = as.character(Fence),
         Fence = ifelse(is.na(Fence), "No Fence", Fence),
         Fence = as.factor(Fence))
#In Regression battleship we had two promises that there was a truth that created why and it obeys all the laws of regression, and that we have all the data used to create the truth. We don't have that.



```


```{r, eval=FALSE}
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)

lm.alley <- lm(SalePrice ~ Alley, data=train)
summary(lm.alley)

lm.alley2 <- lm(SalePrice ~ Alley, data=train2)
summary(lm.alley2)
plot(SalePrice ~ Alley, data=train2)

lm.fence <- lm(SalePrice ~ Fence, data=train)
summary(lm.fence)
```


```{r, eval=FALSE}
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)

lm.2ndflr <- lm(SalePrice ~ X2ndFlrSF, data=train)
summary(lm.2ndflr)

lm.basement <- lm(SalePrice ~ TotalBsmtSF, data=train)
summary(lm.basement)

#Put them all together into a high dimensional multiple regression model

lm.sqft.all <- lm(SalePrice ~ X1stFlrSF + X2ndFlrSF + TotalBsmtSF, data=train)
summary(lm.sqft.all)

#Or use mutate and create a new "TotalSF" variable that allows for a simple linear regression model that is just as powerful, but far easier to graph and interpret.

train <- train %>%
  mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF)

lm.sqft <- lm(SalePrice ~ TotalSF, data=train)
summary(lm.sqft)
 # This is isn't as accurate but it's more interpretable.
 

#Notice as well how mutate can be used to change complicated variables into simple on-off switches.

plot(SalePrice ~ Neighborhood, data=train, las=2, cex.axis=0.5)

train <- train %>%
  mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,
                RichNbrhd = case_when(Neighborhood %in% c("StoneBr", "NridgHt", "NoRidge") ~ 1,                                               TRUE ~ 0))

lm.sqft.rich <- lm(SalePrice ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
summary(lm.sqft.rich)

plot(SalePrice ~ TotalSF, data=train, col=as.factor(RichNbrhd))

b <- coef(lm.sqft.rich)
b

drawit <- function(RichNbrhd, i) {
  curve(b[1] + b[2]*TotalSF + b[3]*RichNbrhd + b[4]*TotalSF*RichNbrhd, add=TRUE, xname="TotalSF", col=palette()[i])
}

drawit(RichNbrhd = 0, 1)
drawit(RichNbrhd = 1, 2)

legend("topleft", legend=c("Regular Nbrhd", "Rich Nbrhd"), lty=1, col=palette()[1:2], cex=0.8)
```


```{r, eval=FALSE}
lm.sqft <- lm(SalePrice ~ TotalSF, data=train)
summary(lm.sqft)
plot(lm.sqft, which=1)

library(car)
boxCox(lm.sqft)

lm.sqft.log <- lm(log(SalePrice) ~ TotalSF, data=train)
summary(lm.sqft.log)
plot(lm.sqft.log, which=1)


plot(lm.sqft$residuals ~ ., data=train)

#OverallQual
#Neighborhood

lm3d <- lm(SalePrice ~ TotalSF + OverallQual + TotalSF:OverallQual, data=train)
summary(lm3d)

plot(SalePrice ~ OverallQual, data=train)
plot(SalePrice ~ TotalSF, data=train)


```

```{r, eval=FALSE}
library(reshape2)
library(plotly)
#Perform the multiple regression
lm3d <- lm(SalePrice ~ TotalSF + I(TotalSF^2) + OverallQual + TotalSF:OverallQual, data=train)


#Setup Axis
axis_x <- seq(min(train$TotalSF), max(train$TotalSF), by = 100)
axis_y <- seq(min(train$OverallQual), max(train$OverallQual), by = 1)

#Sample points
air_surface <- expand.grid(TotalSF = axis_x, OverallQual = axis_y, KEEP.OUT.ATTRS=F)
air_surface <- air_surface %>% mutate(Z=predict.lm(lm3d, newdata = air_surface))
air_surface <- acast(air_surface, OverallQual ~ TotalSF, value.var = "Z") #y ~ x

#Create scatterplot
plot_ly(train, 
        x = ~TotalSF, 
        y = ~OverallQual, 
        z = ~SalePrice,
        text = rownames(train), 
        type = "scatter3d", 
        mode = "markers") %>%
  add_trace(z = air_surface,
            x = axis_x,
            y = axis_y,
            type = "surface")
```



```{r, eval=FALSE}
plot(SalePrice ~ TotalSF, data=train, col=as.factor(OverallQual))

b <- coef(lm3d)

drawit <- function(OverallQual=1, YearBuilt=1980, i=1){
  curve(b[1] + b[2]*TotalSF + b[3]*TotalSF^2 + b[4]*OverallQual + b[5]*TotalSF*OverallQual, add=TRUE, xname="TotalSF", col=palette()[i])
}

drawit(1, 1)
drawit(2, 2)
drawit(3, 3)
drawit(4, 4)
drawit(5, 5)
drawit(6, 6)
drawit(7, 7)
drawit(8, 8)
drawit(9, 1)
drawit(10, 2)

ggplot(train, aes(x=TotalSF, y=SalePrice, color=as.factor(OverallQual))) + 
  geom_point() + 
  facet_wrap(~as.factor(OverallQual))

# My model breaks down at 7000 sqft, so if it's above 7k then it's better to use the average which is 2500
train <- train %>%
  mutate(TotalSF2 = ifelse(TotalSF > 7000, 2500, TotalSF))

# Think mutate mutate mutate
```


```{r, eval=FALSE}
library(tidyverse)
library(mosaic)
plot(hp ~ qsec, data=mtcars, col=as.factor(cyl))

table(mtcars$cyl)

lm.mtcars <- lm(hp ~ qsec + as.factor(cyl) + qsec:as.factor(cyl), data=mtcars)
summary(lm.mtcars)
b <- coef(lm.mtcars)


drawit <- function(cyl6=0, cyl8=0, i=1){
  curve(b[1] +b[2]*qsec + b[3]*cyl6 + b[4]*cyl8 + b[5]*qsec*cyl6 + b[6]*qsec*cyl8, add=TRUE, xname = "qsec", col=palette()[i])
}

# both 0 is 4 cyl
cyl6=0
cyl8=0
drawit(0, 0, 1)
drawit(1, 0, 2)
drawit(0, 1, 3)

#as.factor(cyl) makes on off switches in the  background as.factor(cyl)6, but if you want more control you have to make them yourself



library(dplyr)
View(starwars)
star.lm <- lm(mass ~ height + species, data=starwars)
summary(star.lm)

plot(mass ~ height, data=starwars, col=as.factor(species))
b <- coef(star.lm)

for (i in 3:32){
  curve(b[1] + b[2]*x + b[i], add=TRUE)
}

ggplot(starwars, aes(x=height, y=mass, color=as.factor(species)))


library(car)
library(MASS)
# SIMPLE Linear Regresssion or OLS ordinary least squares
lm.dav <-lm(weight~repwt, data=Davis)
summary(lm.dav)

plot(weight~repwt, data=Davis)
abline(lm.dav)
# robust regression, MASS pkg
rlm.dav <- rlm(weight~repwt, data=Davis)
summary(rlm.dav)
# no nmesuremnt on significance or r-squared fit
abline(rlm.dav, col="red")
# if you use rlm, you need to calculate r-squared
# I use it to see whether the model is being biased

par(mfrow=c(2,2), mai=c(.5,.5,.5,.1))

plot(lm.dav, which=c(1,5))

plot(rlm.dav, which=c(1,5))

#stat note-> multiple linear regression -> outlier analysis
# measure distance between two lines is SSR
# Cooks distance the difference between the two lines (lm & rlm) or distance when a point is removed (then diff in yhats)
# the leverage is the x axis on Residuals v Leverage plot
# smaller cooks distance at edge has a greater innpact on line
# watch for if the d ot is in the middle of tthe data not as big of a deal, but it is on edge
# as long as no interactions or squared terms 


```


#### 11. Logistic

The steeper the curve the more insight it's giving you, if it's shallower its less
asymptote never touches 0 or 1

Odds can become infinite
probability always between 0 and 1

odds are ratio of success to failure
probability ration of success to sample size
If shoot 10 baskets and make 3
prob is 3/10 or .3
odds are 3/7 or .42

5 out of 10
prob 5/10 or .5 (50/50)
odds 5/5 or 1   one success to one failure

8 out of 10
8/10 prob or .8
odds 8/2 or 4 four success to one failure

e to B is the baseline odds 

Mathematically if etob1 is .79 every time I increase the odds by 1 i'm multiplying by .79 .79 times as likely or 21% decreaes

dcrease by a factor of .79 for every 1 degree increase in tempurature

Men are 3 times more likely to have a heart attack. some one did logistic regression and found e to b1 was 3

4.2 co 4.2 times more likely to have success

DO NOT FORGET THE CONTAINING PARENTHESIS AROUND THE DENOMINATOR

the intercept is the first number above e and the temp (beta) is the second number
multiple regression slap them in

if there's a 1. something the the .somethingh is 1] 1.15493

```{r, eval=FALSE}
pred <- predict(chall.glm, data.frame(temp=31), type='response')
# if type left off it's the odds
# when type is response it's the probability


library(car)
View(Davis)

plot(height ~ weight, data=Davis)
#remove point
plot(height ~ weight, data=Davis[-12,])

lm1 <- lm(height ~ weight, data=Davis[-12,])

abline(lm1)


plot(height > 180 ~ weight, data=Davis[-12,])
# like drawing a line and putting dots above the line at top and bottom on the bottom

glm1 <- glm(height > 180 ~ weight, data=Davis[-12,], family=binomial)
summary(glm1)
# then exp the beta
exp(0.14404)
# they are 1.15493 times as likely to be over 180 cm for every of 1 kg of weight they gain . people who are 1kg heavier than others are about 1.15 times (or 15%) more likely to be above 180

library(ResourceSelection)
hoslem.test(glm1$y, glm1$fitted.values)
# backward
# null hypothesis is that your idea to use the test was a good one, so we want to keep it, we want a high number
```



Residuals don't matter with logistic regression
Fisher Scoring guesses and checks, it takes 6 ties to find the best fitting model sometimes never converge

Whats the probability I can buy a house in budget (200k)


What does b0 do (not y intercept), but does control where y intercept lands (have to do e to b0 divided by 1 + e to b0 to get intercept)
What does b1 do

if we add a quadratic term it creates a valley

a cubic can make a stair step


```{r, eval=FALSE}
library(mosaic)
library(tidyverse)

View(Utilities)

plot(gasbill ~ month, data=Utilities)
lm.quad <- lm(gasbill ~ month + I(month^2), data=Utilities)
b <- coef(lm.quad)
curve(b[1] + b[2]*x + b[3]*x^2, add=TRUE)

# convert to logistic by saying i can only afford 80 on my gas bill
plot(gasbill>80 ~ month, data=Utilities, col=rgb(.1,.1,.1, .05), 
     pch=16, cex=2)
glm.quad <- glm(gasbill>80 ~ month + I(month^2), data=Utilities, 
                family=binomial)
b <- coef(glm.quad)
curve(exp(b[1] + b[2]*x + b[3]*x^2)/(1 + exp(b[1] + b[2]*x + b[3]*x^2)), 
      add=TRUE)



ggplot(Utilities, aes(x=month, y=as.numeric(gasbill>80))) + 
  geom_point(alpha=0.05, size=5)



# two lines model
plot(mpg ~ qsec, data=mtcars, col=as.factor(am))
lm.2l <- lm(mpg ~ qsec + am + qsec:am, data=mtcars)
b <- coef(lm.2l)

x2=0
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE)

x2=1
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="red")


plot(mpg>20 ~ qsec, data=mtcars, col=as.factor(am))
glm.2l <- glm(mpg>20 ~ qsec + am + qsec:am, data=mtcars, family=binomial)
b <- coef(glm.2l)

x2=0
curve(exp(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2)/
        (1+exp(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2)), add=TRUE)

x2=1
curve(exp(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2)/
        (1+exp(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2)), add=TRUE, col="red",
      n=1000)
# gets exponential of all coefi
exp(b)

#### He needs the logistic version of the multiple regression

# +1 moves the decimal to the right 1, - moves it to the left

# we draw the probability but interpret odds. (family=binomial)
### Don't forget the number your looking for is not B  IT'S EXP of B

# in the summary intcpt qsec am ec.. are the odds. the am num is the switch from auto to manual (you multiply qsec by the am?)

# qsec is the x axis the am could be femail if the exp of that is 3.1 then 3.1 times more likely
# The on/off switch tells us the shift in the two when we switch groups
# if a change in slope allowed have to calculate exp(b[2]) and exp(b[2] + b[4])

# after ~ is b1 then b2 then b3 (b0 intcpt)
# when indexing though since it starts at 1 the numbers are one higher

```


```{r, eval=FALSE}
library(plotly)
library(reshape2)
View(airquality)
air_lm <- glm(Ozone>80 ~ Temp + Month, data=airquality, family=binomial)

```


```{r, eval=FALSE}
# To embed the 3d_scatterplot inside of your html document is harder.
library(plotly)
library(reshape2)

#Perform the multiple regression
air_lm <- glm(Ozone>80 ~ Temp + Month, data=airquality, family=binomial)# add plus temp month

#Graph Resolution (more important for more complex shapes)
graph_reso <- 0.5
# 
#Setup Axis
axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by=graph_reso)
axis_y <- seq(min(airquality$Month), max(airquality$Month), by=graph_reso)

#Sample points
air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS = F)
air_surface$Z <- predict.glm(air_lm, newdata=air_surface, type="response")###type imp
air_surface <- acast(air_surface, Month ~ Temp, value.var = "Z")

#Create scatterplot
plot_ly(airquality,
        x= ~Temp,
        y= ~Month,
        z= ~as.numeric(Ozone>80),
        text = rownames(airquality),
        type= "scatter3d",
        mode = "markers") %>%
  add_trace(z= air_surface,
            x = axis_x,
            y= axis_y,
            type = "surface")

```


```{r, eval=FALSE}

# To embed the Hd_scatterplot inside of your html document is harder.
library(plotly)
library(reshape2)

#Perform the multiple regression
air_lm <- glm(Ozone>80 ~ Wind + Temp, data=airquality, family=binomial)# add plus temp month

summary(air_lm)

#Graph Resolution (more important for more complex shapes)
graph_reso <- 0.5
# 
#Setup Axis
axis_x <- seq(min(airquality$Temp), max(airquality$Temp), by=graph_reso)
axis_y <- seq(min(airquality$Month), max(airquality$Month), by=graph_reso)

#Sample points
air_surface <- expand.grid(Temp = axis_x, Month = axis_y, KEEP.OUT.ATTRS = F)
air_surface$Z <- predict.glm(air_lm, newdata=air_surface, type="response")###type imp
air_surface <- acast(air_surface, Month ~ Temp, value.var = "Z")

#Create scatterplot
plot_ly(airquality,
        x= ~Temp,
        y= ~Month,
        z= ~as.numeric(Ozone>80),
        text = rownames(airquality),
        type= "scatter3d",
        mode = "markers") %>%
  add_trace(z= air_surface,
            x = axis_x,
            y= axis_y,
            type = "surface")



```


How much information is the model capturing

p is parmeter in AIC



