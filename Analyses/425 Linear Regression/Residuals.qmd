---
title: "Regression Theory"
format:
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.height=4.5, fig.width=4)

library(tidyverse) 
library(data.table)
library(latex2exp)
library(ggbrace)
library(mosaic) 
library(DT)

theme_set(theme_minimal()) 
```

Basic terms of linear regression will be described using the data below which was collected from [timeanddate.com](#0). Please note that some squares near the right edge that would go off the plot are not drawn.

```{r, data}
weather_hist <- read.csv("0110_Weather.csv")
weather_hist <- rename(weather_hist, PrevBar = bar.1)
weather_hist <- rename(weather_hist, TempTwoAhead = Temp...2)
ybar <- mean(weather_hist$Temp)
actual <- 39
mylm <- lm(TempTwoAhead~Temp, data=weather_hist)
y_edgex <-predict(mylm, data.frame(Temp=7))

# datatable 
datatable(weather_hist, options=list(lengthMenu = c(3,10,30)), extensions="Responsive")
```

<br>

The plot below uses a regression on the temperature data, colored in gold, to predict the high temperature on January 12th, 2026.

```{r, baseplot}
# create a base plot
p <- ggplot(weather_hist, aes(y=TempTwoAhead, x=Temp)) +
coord_cartesian(xlim = c(NA, 37.2)) +
geom_hline(yintercept = ybar, color="darkgray") +
geom_point(color="gold") +
geom_smooth(method="lm", se=F, formula=y~x, color="lightblue", alpha=0.25, 
            fullrange=TRUE) +
scale_x_continuous(limits = c(min(weather_hist$Temp)-1, 37.2), expand = c(0, 0)) +
annotate("point", x=32, y=28.33, color="dodgerblue4") +
annotate("point", x=32, y=actual, color="orange") +
annotate("text", x=33.2, y=28.8, 
          label=TeX("$\\hat{Y} = 28^{\\circ}F$"), color="dodgerblue4",
          linewidth=3) +
annotate("text", x=33.2, y=39.22, 
          label=TeX("$Y_i = 39^{\\circ}F$"), color="darkorange",
          linewidth=3) +
annotate("text", x=37, y=ybar-1.1, 
          label=TeX("$\\bar{y}$"), color="gray60",
          linewidth=3) +
labs(
  title = "Predicted Weather for Rexburg on January 12th",
  x = "High Temperature in \u00B0F Two Days Before with Similar Conditions",
  y = "High Temperature in \u00B0F") #+
 # coord_equal(ratio=1, ylim=ylim)

p
```

The predicted high or $\hat{Y_i}$ is marked in dark blue, while the actual observed temperature, $Y_i$ , is marked in orange (because it was not used to create the regression). The subscript $i$ stands for individual. The regression line, $b_0 + b_1$, is marked in blue. The dark gray line marks $\bar{y}$, the mean of the data. The true line is unknown, so it is not on the plot.

<br>

## Residual, $r_i$

The dark blue dots represent the individual Y's estimated by the regression. It is assumed that the prediction of 28 degrees is correct, so the recorded high temperature for January 12th of 39 degrees is 11 degrees off.

```{r, residualplot}
p +
geom_segment(aes(x=Temp, xend=Temp, y=TempTwoAhead, yend=mylm$fit), color="dodgerblue2") +
annotate("segment", x=32, xend= 32, y=28.33, yend=actual, 
          color="dodgerblue2") +
stat_brace(
  data = data.frame(x = c(32.2, 32.2), y = c(28.6, actual-0.3)), # Vertical span
  aes(x, y),
  rotate = 90, # 90 for vertical
  color = "dodgerblue3",
  alpha = 0.5,
  linewidth = .5) +
stat_bracetext(
  data = data.frame(x = c(32.2, 32.2), y = c(28.6, actual-0.3)),
  aes(x, y),
  label = TeX("$r_i=11$"),
  rotate = 90,
  color = "dodgerblue2")
```

11 degrees is a residual. It's an error term representing how far the actual temperature ($Y_i$) is from the prediction ($\hat{Y_i})$ :

$$
r_i = Y_i-\hat{Y_i}
$$

<br>

For an lm() the sum of the residuals will always be 0. Residuals are always in a vertical direction because the x is not changing. They can be used to show the variation in $Y_i$ not explained by the regression. The residuals for the regression line in the plot above can be found with the code below.

```{r, residual}
#| code-fold: show
# mylm was created in "data" code block
# pull the residuals from the model object, showing 3 for neatness
head(mylm$residuals,3)

```

*The residuals give us the line. \~Bro. Saunders*

<br>

## Sum of Squared Errors, $SSE$

The shaded regions below are squares with all sides the length of the corresponding residual.

```{r, SSEplot}
p +
geom_point(aes(x=Temp, y=mylm$fit), color = "dodgerblue3") +
geom_rect(aes(xmin=Temp, xmax=Temp - mylm$res*0.44, 
              ymin=mylm$fit, ymax=TempTwoAhead), 
          color="dodgerblue2", alpha=0.1, fill="dodgerblue2") +
annotate("rect", xmin=32, xmax=32 - 11*0.44, ymin=28.33, ymax=39, 
         color="dodgerblue2", alpha=0.1, fill="dodgerblue2") +
stat_brace(
  data = data.frame(x = c(32.1, 32.1), y = c(28.6, actual-0.3)), # Vertical span
  aes(x, y),
  rotate = 90, # 90 for vertical
  color = "dodgerblue3",
  alpha = 0.5,
  linewidth = .5) +
stat_bracetext(
  data = data.frame(x = c(32.1, 32.1), y = c(28.6, actual-0.3)),
  aes(x, y),
  label = TeX("$Y_i-\\hat{Y_i}$"),
  rotate = 90,
  color = "dodgerblue2") 

# note: the square for the data point close to the left edge isn't showing because it's going off the edge of the lm's left-hand limit

```

Since adding the residuals will always equal 0, the sum of squaring the residuals is more useful. Squaring the residuals eliminates negative values, so the total error of the model can be calculated. This is called the Sum of Squared Errors (because a residual is an error term) and measures how well the observed values match the predicted values.

$$ SSE = \sum_{i=1}^{n}{(\underbrace{Y_i}_{Actual Temp}-\underbrace{\hat{Y_i}}_{Predicted Temp})}^2 $$

Squaring guarantees the $SSE$ will never be negative, $SSE \ge 0$ . This measurement also allows comparison of different models on the same data and represents the portion of the variability that is still unexplained, so the lower the $SSE$, the better. Unfortunately, squaring also means SSE can be greatly skewed by outliers.

The code below calculates the SSE for the linear model of the weather data.

```{r, SSE}
#| code-fold: show
SSE <-sum(mylm$residuals^2)
SSE
```

*Minimizing the quadratic is beautiful, that's why we like squares. \~Bro. Saunders*

<br>

## Sum of Squared Regression, $SSR$

The sum of squared regression indicates how much more explanatory value the regression has over just using the average y value. It indicates how much of the variability in the data we understand.

```{r, SSRplot}
p +
geom_point(aes(x=Temp, y=mylm$fit), color = "dodgerblue3") +
geom_rect(aes(xmin=Temp, xmax=Temp-(mylm$fit-ybar)*0.44,
              ymin = pmin(ybar, mylm$fit), ymax = pmax(ybar, mylm$fit)),
          color="pink2", alpha=0.1, fill="pink1") +
annotate("rect", xmin=32, xmax=32 - 11.7744*0.44, ymin=ybar, ymax=28.33, 
         color="pink1", alpha=0.1, fill="pink1") +
stat_brace(
  data = data.frame(x = c(32.1, 32.1), y = c(ybar+0.3, 28)), # Vertical span
  aes(x, y),
  rotate = 90, # 90 for vertical
  color = "pink2",
  alpha = 0.5,
  linewidth = .5) +
stat_bracetext(
  data = data.frame(x = c(32.1, 32.1), y = c(ybar+0.3, 28)),
  aes(x, y),
  label = TeX("$\\hat{Y_i}-\\bar{Y}$"),
  rotate = 90,
  color = "pink2")

```

The $SSR$ is calculated by squaring the differences between the predicted values and the average value:

$$ SSR = \sum_{i=1}^{n}{(\underbrace{\hat{Y_i}}_{Predicted Temp}-\underbrace{\bar{Y}}_{Average Temp})}^2 $$

Like $SSE$, the sum of square regression can be skewed by outliers and must be greater than or equal to zero, $SSR\ge0$. An $SSR$ of zero means the regression line is the same as the average y. Unlike $SSR$, the better the model, the larger the value because the regression has that much more value than the average y.

The code below calculates the SSR of the the weather data.

```{r, SSR}
SSR <- sum((mylm$fit-mean(weather_hist$TempTwoAhead))^2)
SSR
```

<br>

## Total Sum of Squares, $SSTO$

The Total Sum of Squares is the total variability in the data.

```{r, SSTOplot}
p +
geom_rect(aes(xmin=Temp, xmax=Temp-(TempTwoAhead-ybar)*0.44,
              ymin = pmin(ybar, TempTwoAhead), ymax = pmax(ybar, TempTwoAhead)),
          color="mediumorchid1", alpha=0.1, fill="mediumorchid1") +
annotate("rect", xmin=32, xmax=32 - (39-ybar)*0.44, ymin=ybar, ymax=39, 
         color="mediumorchid1", alpha=0.1, fill="mediumorchid1") +
stat_brace(
  data = data.frame(x = c(32.1, 32.1), y = c(ybar+0.3, actual-0.3)), # Vertical span
  aes(x, y),
  rotate = 270,
  color = "darkorchid",
  alpha = 0.5,
  linewidth = .5) +
stat_bracetext(
  data = data.frame(x = c(32.2, 32.2), y = c(ybar+0.3, actual-0.3)),
  aes(x, y),
  label = TeX("${Y_i}-\\bar{Y}$"),
  rotate = 270,
  color = "darkorchid")
```

The $SSTO$ is calculated by squaring the actual temperatures minus average temp and then adding them together as shown below.

$$
SSTO = \sum_{i=1}^{n}{(\underbrace{{Y_i}}_{Actual Temp}-\underbrace{\bar{Y}}_{Average Temp})}^2
$$

Again, since the differences between the actual temps and the average temp are squared, the $SSTO$ can not be negative, $SSTO\ge0$. The code below finds the $SSTO$ for the data.

```{r, SSTO}
SSTO <- sum((weather_hist$TempTwoAhead-mean(weather_hist$TempTwoAhead))^2)
SSTO
```

<br>

#### Sum of Squares Relationships

The plot below illustrates the relationship between the different types of sums of squares:

```{r, relationships}
p +
# SSE
annotate("rect", xmin=32, xmax=32 + 11*0.44, ymin=28.33, ymax=39, 
         color="dodgerblue2", alpha=0.1, fill="dodgerblue2") +
annotate("text", x=34.42 , y=40.1, label="SSE", color="dodgerblue2", linewidth=4) +
stat_brace(
  data = data.frame(x = c(32.1, 32.1), y = c(28.6, actual-0.3)), # Vertical span
  aes(x, y),
  rotate = 90, # 90 for vertical
  color = "dodgerblue3",
  alpha = 0.5,
  linewidth = .5) +
stat_bracetext(
  data = data.frame(x = c(32.1, 32.1), y = c(28.6, actual-0.3)),
  aes(x, y),
  label = TeX("$Y_i-\\hat{Y_i}$"),
  rotate = 90,
  color = "dodgerblue2",
  linewidth = 3) +
# SSR
annotate("rect", xmin=32, xmax=32 + 11.7744*0.44, ymin=ybar, ymax=28.33, 
         color="pink1", alpha=0.1, fill="pink1") +
annotate("text", x=34.42 , y=ybar-1.1, label="SSR", color="pink1", linewidth=4) +
stat_brace(
  data = data.frame(x = c(32.1, 32.1), y = c(ybar+0.3, 28)), # Vertical span
  aes(x, y),
  rotate = 90, # 90 for vertical
  color = "pink2",
  alpha = 0.5,
  linewidth = .5) +
stat_bracetext(
  data = data.frame(x = c(32.1, 32.1), y = c(ybar+0.3, 28)),
  aes(x, y),
  label = TeX("$\\hat{Y_i}-\\bar{Y}$"),
  rotate = 90,
  color = "pink2",
  linewidth = 3) +
# SSTO
annotate("rect", xmin=32, xmax=32 - (39-ybar)*0.44, ymin=ybar, ymax=39, 
         color="mediumorchid1", alpha=0.1, fill="mediumorchid1") +
annotate("text", x=27.062 , y=40.1, label="SSTO", color="darkorchid", linewidth=4) +
stat_brace(
  data = data.frame(x = c(32.1, 32.1), y = c(ybar+0.3, actual-0.3)), # Vertical span
  aes(x, y),
  rotate = 270,
  color = "darkorchid",
  alpha = 0.5,
  linewidth = .5) +
stat_bracetext(
  data = data.frame(x = c(32.2, 32.2), y = c(ybar+0.3, actual-0.3)),
  aes(x, y),
  label = TeX("${Y_i}-\\bar{Y}$"),
  rotate = 270,
  color = "darkorchid",
  linewidth =3) 
```

$$
SSTO = SSE + SSR
$$

Notice that mixing blue and pink result in a light purple, just as $SSE$ and $SSR$ combine to make $SSTO$.

The maximum value of any one of the sum of squares alone can be any non-negative value as long as the relationships are maintained such that the sum of the $SSE+SSR$ is the same as the $SSTO$. This means that neither the $SSE$ nor the $SSR$ can be greater than the $SSTO$. If the $SRR=SSTO$, then the $SSE$ would be 0. In other words, if $SSR=SSTO$ the regression fit is perfect. If $SSE=SSTO$, then the $SSR$ is 0 making the regression a horizontal line at $\bar{y}$.

<br>

## R-Squared, $R^2$

$R^2$ is the ratio between the the $SRR$ and the $SSTO$. It's the proportion of the variability in $Y_i$ explained by the regression or the strength the relationship between the model and the data. The higher the $R^2$, the stronger the relationship, the more variability explained by the model.

```{r, r2plot}
p +
# SSR
annotate("rect", xmin=32, xmax=32 + 11.7744*0.44, ymin=ybar, ymax=28.33, 
         color="pink1", alpha=0.1, fill="pink1") +
annotate("text", x=34.42 , y=ybar-1.1, label="SSR", color="pink1", linewidth=4) +
# SSTO
annotate("rect", xmin=32, xmax=32 - (39-ybar)*0.44, ymin=ybar, ymax=39, 
         color="mediumorchid1", alpha=0.1, fill="mediumorchid1") +
annotate("text", x=27.062 , y=40.1, label="SSTO", color="darkorchid", linewidth=4)

```

In the plot above the squares are hard to compare because they're different widths. Converting to columns makes it easier.

```{r}
R2plot <- data.frame(label=c('SSTO', 'SSR'), amount=c(SSTO, SSR))
ggplot(data=R2plot, aes(x=label, y=amount, fill=label))+
  geom_col(alpha=0.4) +
  scale_fill_manual(values=c('SSTO'="mediumorchid1", 'SSR'='pink1')) 
```

The largest $R^2$ can be is 1 (the variability is completely explained by the regression), and the smallest is 0 (the explanatory value of the regression is no better than $\bar{Y}$).

$$
R^2 = \frac{SSR}{SSTO}
$$

The $SSR$ and $SSTO$ were stored as variables, so the code is simple.

```{r, r2}
SSR/SSTO
```

<br>

## MSE & Residual Standard Error

The $MSE$ is the mean of the $SSE$ \*. The plot below is the same as the plot previously used to demonstrate $SSE$, except inside every square representing the $SSE$, there's a smaller square which shows the $MSE$.

```{r, MSEplot}
p +
geom_point(aes(x=Temp, y=mylm$fit), color = "dodgerblue3") +
geom_rect(aes(xmin=Temp, xmax=Temp - mylm$res*0.44,
              ymin=mylm$fit, ymax=TempTwoAhead),
          color="dodgerblue2", alpha=0.1, fill="dodgerblue2") +
annotate("rect", xmin=32, xmax=32 - 11*0.44, ymin=28.33, ymax=39, 
         color="dodgerblue2", alpha=0.1, fill="dodgerblue2") +
geom_rect(aes(xmin=Temp, xmax=Temp - (mylm$res*0.44)/7,
              ymin=mylm$fit, ymax=mylm$fit+mylm$residuals/7),
          color="gray60", alpha=0.3, fill="gray60") +
annotate("rect", xmin=32, xmax=32 - (11*0.44)/7, ymin=28.33, ymax=28.33+11/7, 
         color="gray60", alpha=0.3, fill="gray60") 


```

Due to squaring, $MSE \ge0$ . Like the other measures, it's sensitive to outliers and the units are squared. The $MSE$ is different from $R^2$ in that the former is an average while the latter is a proportion. The formula is:

$$
MSE = \frac{SSE}{n-p}
$$

\*Notice that rather than being divided by $n$ , the $SSE$ is divided by $n-p$ where $p$ is the number of parameters. Using the degrees of freedom, $n-p$, reduces bias. For simple linear regression the parameters are $\beta_0, \beta_1$ , so the $SSE$ for the data will be divided by 9-2 or 7.

```{r, MSE}
SSE/7
```

Closely tied to the $MSE$ is the residual standard error, $r$. It's the square root of $R^2$ which allows the values to be from -1 to 1. $r$ is also called the correlation cooeficient:

$$
r=\sqrt{\frac{SSE}{n-p}}
$$

It's easier to interpret because, unlike the various sum of squares and MSE, the units aren't squared. $r$ can be found in the model summary under residual standard error (3rd line up from the bottom).

```{r}
summary(mylm)
```
