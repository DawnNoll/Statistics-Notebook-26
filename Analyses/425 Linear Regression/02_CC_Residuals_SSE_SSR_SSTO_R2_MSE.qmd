---
title: "0112_CC_Residuals_SSE_SSR_SSTO_R2_MSE"
format: html
editor: visual
---

Question; Why is it SSE aren't the true errors unknown? Why isn't it the SSr?

[Regression Applet](https://phet.colorado.edu/sims/html/least-squares-regression/latest/least-squares-regression_en.html)

## Residuals

Residuals are always in a vertical direction (not horizontal). It's always vertical because the x is not changing.

The pink dot is $b_0 + b_1$ or $Y_i$, $r_i = Y_i - \hat{Y_i}$ – we believe our prediction is correct, but the weather (actual) is wrong. It represents a point our estimated line (lm). On the tool, the more we can get the line in the middle the less less the residuals are, so for an lm the sum of the residuals will always be 0 (if there is an intercept?). The residuals can be found with the code Y - mylm\$fit

*Deterministic: if you know enough you could predict where the electron is. Stochastic: even if you know everything, you can't predict where the electron is.*

## Sum of Squared Errors (SSE)

The least squares regression line is the best fit line.

Sum of Squared Errors measures how much the residuals deviate from the line. Equals SSTO-SSR and can be found using sum( (Y-mylm\$fit)\^2

$$
SSE = \sum_{i=1}^{n}{(Y_i-\hat{Y_i})}^2
$$

```{r}
library(tidyverse)

cars2 <- cars %>%
  filter(speed < 15)

mylm <- lm(dist ~ speed, data=cars2)


ggplot(cars2, aes(x=speed, y=dist)) + 
  geom_point() + 
  geom_smooth(method="lm", se=F, formula=y~x) + 
  geom_rect(aes(xmin=speed, xmax=speed + mylm$res*0.4, ymin=mylm$fit, ymax=dist), alpha=0.1, color="pink") +
  geom_segment(aes(x=speed, xend=speed, y=dist, yend=mylm$fit), color="darkgray")
```

\*Minimizing the quadratic is beautiful, that's why we like squares.\*

## Code for class activity sum of squares and r\^2

```{r}
x = c(5, 15, 2, 29, 35, 24, 25, 39)
sum(x)
sum((1:6)^2)
sum(x^2)
mean(x)
xmean<-mean(x)
sum((x-xmean)^2)/(8-1) # don't forget parenthes on fractions
var(x) # var is for a sample so the denominator is x-1
mean(cars$dist)

library(plotly)
cars.lm <- lm(dist ~ speed, data=cars)
summary(cars.lm)
cars2 <- cbind(rbind(cars, 
                     cbind(speed=rep(0,50), dist=cars$dist),
                     cbind(speed=cars$speed, dist=cars.lm$fitted.values)),
               frame = rep(c(2,1,3), each=50))
plot_ly(cars2,
        x = ~speed,
        y = ~dist,
        frame = ~frame,
        type = 'scatter',
        mode = 'markers',
        showlegend = F,
        marker=list(color="firebrick")) %>%
  layout(title="Stopping Distance of 1920's Vehicles\n (cars data set)",
         xaxis=list(title="Vehicles Speed in mph (speed)"),
         yaxis=list(title="Stopping Distance in Feet (dist)")) %>%
  add_segments(x = 0, xend = 25, y = mean(cars$dist), yend = mean(cars$dist), line=list(color="gray", dash="dash", width=1), inherit=FALSE, name=TeX("\\bar{Y}"), showlegend=T) %>%
  add_segments(x = 0, xend = 25, y = sum(coef(cars.lm)*c(1,0)), yend = sum(coef(cars.lm)*c(1,25)), line=list(color="darkgray", width=1), inherit=FALSE, name=TeX("\\hat{Y}"), showlegend=T) %>%
  config(mathjax = 'cdn') %>%
  animation_opts(frame=2000, transition=1000, redraw=T)

sum(cars.lm$residuals^2)
sum((cars.lm$fit-mean(cars$dist))^2)
sum((cars$dist-mean(cars$dist))^2)
sum((cars.lm$fit-mean(cars$dist))^2)/sum((cars$dist-mean(cars$dist))^2)
sqrt(sum((cars.lm$fit-mean(cars$dist))^2)/sum((cars$dist-mean(cars$dist))^2))

# find for one x
plot(dist ~ speed, data = cars)
points(dist ~ speed, data=cars[23,], col="orange", pch=16)

# other useful
cars[23, ]
cars.lm$residuals[23] # indexs the 23rd residuals from the lm obj
cars$dist[23] - cars.lm$fit[23]
```

$r$ is the coorelation coefficient. THe regression doesn't help us understand the data. It helps us understand the variability in the data. It's helping us understand the variability in the data (because the data isn't falling on the line–remember we're right, the data is wrong)

pvalue measures whether there is a law, rquared measures how obedient the data is to the law

p value doesnt' tell you whether something is true, just whether it's significant

R2 is the proportion of the variation in Y explained by the regression model

MSE is like the variance because were kind of averaging a some of squares, specifically the SSE/n-p

p for now is 2 because there are two parameters beta0, beta1

The sqare root of MSE is the residual standard error, it estimates the sigma of the dots (how far the dots are from the line)
