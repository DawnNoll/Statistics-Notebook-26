---
title: "SamplingDistributionsUnveiled"
format:
  html:
    embed-resources: true
    code-fold: true
editor: visual
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.height=4.5, fig.width=4)

library(tidyverse) 
library(data.table)
library(latex2exp)
library(ggbrace)
library(mosaic) 
library(DT)

theme_set(theme_minimal()) 
```

## Sampling Distributions of the Slope and Intercept of a Regression Line

A sampling distribution is what you get when you take many samples of the same population. It essentially allows us to find the mean of many means and the standard deviation of many standard deviations. Our understanding of sampling distributions allow us to infer what is true for population.

```{r, eval=FALSE, fig.height=8, fig.width=8}
set.seed(616)

 
n <- 100 #sample size (choose a decent size sample)
# spread of x (choose a decent spread)
Xstart <- 5 #lower-bound for x-axis 
Xstop <- 300 #upper-bound for x-axis

# set the law
beta_0 <- 32   #system and base apps/true y-intercept
beta_1 <- 2 #choice of true slope
sigma <- 3.4   #choice of sd


#Create X - this way most insight for least data 
X <- rep(seq(Xstart,Xstop, length.out=n/2), each=2) 
N <- 5000 #number of times to pull a random sample (for the assignment it's 3)

# create storage first for efficiency
storage_b0 <- storage_b1 <- rep(NA, N)

for (i in 1:N){
  Y <- beta_0 + beta_1*X + rnorm(n, 0, sigma) #Sample Y from true model
  mylm <- lm(Y ~ X)
  storage_b0[i] <- coef(mylm)[1]
  storage_b1[i] <- coef(mylm)[2]
}

# only works if you're using base r
layout(matrix(c(1,1,2,3), 2, 2, byrow = TRUE), widths=c(2,2), heights=c(3,3))

Ystart <- 0 #min(0,min(Y)) 
Ystop <- 500 #max(max(Y), 0)
Yrange <- Ystop - Ystart
# need plot and histograms, the rest is extra
plot(Y ~ X, xlim=c(min(0,Xstart-2), max(0,Xstop+2)), 
     ylim=c(Ystart, Ystop), pch=16, col="gray",
     main="Regression Lines from many Samples\n Plus Residual Standard Deviation Lines")
text(Xstart, Ystop, bquote(sigma == .(sigma)), pos=1)
text(Xstart, Ystop-.1*Yrange, bquote(sum ((x[i]-bar(x))^2, i==1, n) == .(var(X)*(n-1))), pos=1)



# for (i in 1:N){
#   abline(storage_b0[i], storage_b1[i], col="darkgray")  
# }
# abline(beta_0, beta_1, col="green", lwd=3)
# abline(beta_0+sigma, beta_1, col="green", lwd=2)
# abline(beta_0-sigma, beta_1, col="green", lwd=2)
# abline(beta_0+2*sigma, beta_1, col="green", lwd=1)
# abline(beta_0-2*sigma, beta_1, col="green", lwd=1)
# abline(beta_0+3*sigma, beta_1, col="green", lwd=.5)
# abline(beta_0-3*sigma, beta_1, col="green", lwd=.5)
# 
# par(mai=c(1,.6,.5,.01))
# 
#   addnorm <- function(m,s, col="firebrick"){
#     curve(dnorm(x, m, s), add=TRUE, col=col, lwd=2)
#     lines(c(m,m), c(0, dnorm(m,m,s)), lwd=2, col=col)
#     lines(rep(m-s,2), c(0, dnorm(m-s, m, s)), lwd=2, col=col)
#     lines(rep(m-2*s,2), c(0, dnorm(m-2*s, m, s)), lwd=2, col=col)
#     lines(rep(m-3*s,2), c(0, dnorm(m-3*s, m, s)), lwd=2, col=col)
#     lines(rep(m+s,2), c(0, dnorm(m+s, m, s)), lwd=2, col=col)
#     lines(rep(m+2*s,2), c(0, dnorm(m+2*s, m, s)), lwd=2, col=col)
#     lines(rep(m+3*s,2), c(0, dnorm(m+3*s, m, s)), lwd=2, col=col)
#     legend("topleft", legend=paste("Std. Error = ", round(s,3)), cex=0.7, bty="n")
#   }
# # All you really need is a histogram in storage
#   h0 <- hist(storage_b0, 
#              col="skyblue3", 
#              main="Sampling Distribution\n Y-intercept",
#              xlab=expression(paste("Estimates of ", beta[0], " from each Sample")),
#              freq=FALSE, yaxt='n', ylab="")
#   m0 <- mean(storage_b0)
#   s0 <- sd(storage_b0)
#   addnorm(m0,s0, col="green")
#   
#   h1 <- hist(storage_b1, 
#              col="skyblue3", 
#              main="Sampling Distribution\n Slope",
#              xlab=expression(paste("Estimates of ", beta[1], " from each Sample")),
#              freq=FALSE, yaxt='n', ylab="")
#   m1 <- mean(storage_b1)
#   s1 <- sd(storage_b1)
#   addnorm(m1,s1, col="green")
# 
# ### Monday
# X <- rnorm(N, 30, 5)
# Y <- beta0 + beta1*X + rnorm(N, 0, sigma)
# 
# plot(Y ~ X, col=rgb(.1,.1,.1,.01), pch=16) #rgb(red, green, blue, alpha)
# 
# n <- 5
# mysample1 <- sample(N, n)
# 
# points(Y[mysample1] ~ X[mysample1], col="hotpink", pch=16)
# mylm1 <- lm(Y[mysample1] ~ X[mysample1])
# summary(mylm1)
# abline(mylm1, col="hotpink")
# abline(beta0, beta1, col="green1")
# 
# n <- 5
# mysample2 <- sample(N, n)
# 
# points(Y[mysample2] ~ X[mysample2], col="orange", pch=16)
# mylm1 <- lm(Y[mysample2] ~ X[mysample2])
# summary(mylm1)
# abline(mylm1, col="orange")
# abline(beta0, beta1, col="green1")

```

-   A researcher investigated whether the number of cute animal photos stored on a phone could predict the total storage used.

    $$ \underbrace{Y_i}_\text{Storage Used in GB} = \overbrace{\beta_0}^\text{`r beta_0` GB} + \overbrace{\beta_1}^\text{`r beta_1` } \underbrace{X_i}_\text{Photos} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0,\overbrace{\sigma^2}^\text{`r sigma` GB^2}) $$

    ```{}
    ```

-   Show scatterplots and regression summary's for three different samples of data (of a given sample size) from your population. *shows all samples are wrong and none agree*

-   Demonstrate how the sampling distribution is obtained for several thousand samples of both the slope and the intercept estimates and in your regression context.

-   Clearly explain what the mean and standard deviation (often called the standard error) are for each of these two distributions.

-   Most importantly, provide thorough explanation of what the standard error of your three summary output's from before measure with regards to the sampling distributions.

-   Show how the standard error is effected by the (1) sample size, (2) the value of sigma you choose, and (3) the range of the x-values. maybe 3 plots with differennt numbers

<br>

## P-values

-   Demonstrate how the **standard errors** of the sampling distributions for both the slope and intercept estimates are used to obtain p-values for the tests of hypotheses about andÂ (the true intercept and slope of the regression model).

-   Discuss why this works.

-   Reveal the logic behind the p-value.

-   Show the equation of the t-value and how to use the pt(...) function in R to convert a t-value to a two-sided p-value.

-   Show that you can calculate yourself the p-value shown in one of your summary outputs from above.

<br>

## Confidence Intervals

-   Demonstrate how the **standard errors** of each sampling distribution are used to create confidence intervals for the true regression intercept, , and true regression slope, .

-   Explain what a confidence interval really is... explain why it captures the true parameter values "95% of the time."

-   Show how to use the qt(...) function in R to obtain the critical value t\* of the confidence interval.

-   Show the equation of the confidence interval and explain how it is using the standard error of the sampling distribution.

-   Match the results of confint(mylm) using qt(...) and the standard error from one of your regression summary outputs to calculate the same confidence interval yourself.
